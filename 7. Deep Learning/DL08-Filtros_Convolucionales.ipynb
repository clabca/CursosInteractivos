{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"hhDpBN9PgCu7"},"source":["# Deep Learning\n","# DL08 Filtros Convolucionales\n"]},{"cell_type":"markdown","metadata":{"id":"63fth2s3Hbhb"},"source":["## <font color='blue'>**Capa convolucional.**</font>\n","<p style='text-align: justify;'>\n","\n","En este notebook, visualizamos cuatro salidas filtradas (también conocidas como mapas de activación) de una capa convolucional.\n","\n","En este ejemplo, *nosotros* estamos definiendo cuatro filtros que se aplican a una imagen de entrada inicializando los **pesos** de una capa convolucional, pero una CNN entrenada, la red aprenderá los valores de estos pesos.\n","\n","![OverFitting](https://drive.google.com/uc?export=view&id=1tyI9a8do69Qzi9O0jQwZAsylz0lDIUfk)"]},{"cell_type":"markdown","source":["La operación de convolución en capas 2D implica tomar un filtro (o kernel) y deslizarlo sobre la imagen de entrada. En cada posición del filtro sobre la imagen, se realiza una suma ponderada de los píxeles de la imagen que se encuentran debajo del filtro, y el resultado se coloca en la imagen de salida en la posición correspondiente.\n","\n","\n","Si $ I $ es la imagen y $ K $ es el kernel, entonces la convolución en una posición específica $ (x, y) $ se define como:\n","\n","\n","$$\n","\\begin{equation}\n","(I * K)(x, y) = \\sum_m \\sum_n I(x + m, y + n)K(m, n)\n","\\end{equation}\n","$$\n","\n","\n","donde las sumas se realizan sobre el ancho y alto del kernel.\n","\n","$\\textbf{Ejemplo de cálculo}:$\n","\n","Supongamos que tenemos una pequeña imagen 3x3 y un filtro 2x2.\n","\n","Imagen:\n","\n","$ I = \\begin{bmatrix}\n","1 & 2 & 3 \\\\\n","4 & 5 & 6 \\\\\n","7 & 8 & 9 \\\\\n","\\end{bmatrix} $\n","\n","Filtro (kernel):\n","\n","\n","$\n","K = \\begin{bmatrix}\n","-1 & 1 \\\\\n","2 & -2 \\\\\n","\\end{bmatrix}\n","$\n","\n","Para calcular la convolución en la posición $ (0,0) $ de la imagen de salida, tomamos la parte superior izquierda de la imagen que se superpone con el filtro y realizamos la suma ponderada:\n","\n","$\n","\\begin{equation}\n","(I * K)(1,1) = 1(-1) + 2(1) + 4(2) + 5(-2) = 2\n","\\end{equation}\n","$\n","\n","El proceso se repite para el siguiente píxel deslizando el filtro a la derecha y luego hacia abajo.\n","\n","El resultado para toda la imagen sería:\n","\n","$ O = \\begin{bmatrix}\n","2 & 2 \\\\\n","2 & 2 \\\\\n","\\end{bmatrix} $\n","\n","Tenga en cuenta que, debido al tamaño del filtro, la imagen resultante es más pequeña que la imagen original en este ejemplo. Sin embargo, en la mayoría de las implementaciones modernas (como en PyTorch\n"],"metadata":{"id":"hW-zB-rD6a1i"}},{"cell_type":"markdown","source":["![Convolucion](https://drive.google.com/uc?export=view&id=1SVRk5jwmBM_g_JhWgDUEbsc1B5k7oyVM)"],"metadata":{"id":"0qk_HrClkWhc"}},{"cell_type":"markdown","metadata":{"id":"hMY4OTtwgCvH"},"source":["### Importamos la imagen."]},{"cell_type":"code","metadata":{"id":"-2X2ui_pPFrS"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PtcNwNAegCvI"},"source":["import cv2\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# TODO: Siéntase libre de probar sus propias imágenes aquí cambiando img_path\n","# a una ruta de archivo a otra imagen en su computadora.\n","\n","\n","img_path = \"/content/drive/MyDrive/Curso/Industria Inteligente/2023-2S/Datos/Auto.jfif\"\n","# load color image\n","bgr_img = cv2.imread(img_path)\n","# convert to grayscale\n","gray_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2GRAY)\n","\n","# normalize, rescale entries to lie in [0,1]\n","gray_img = gray_img.astype(\"float32\")/255\n","\n","# plot image\n","plt.imshow(gray_img, cmap='gray')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dFeSeCnJgCvK"},"source":["### Definamos los filtros."]},{"cell_type":"code","metadata":{"id":"Ug6Ads14gCvK"},"source":["import numpy as np\n","\n","## Se puede modificar libremente. Estamos definiendo el operador de convolución.\n","filter_vals = np.array([[-1, -1, 1, 1], [-1, -1, 1, 1], [-1, -1, 1, 1], [-1, -1, 1, 1]])\n","\n","print('Filter shape: ', filter_vals.shape)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VF-3MgPngCvL"},"source":["# Definiendo 4 filtros. Obtenidos desde `filter_vals`\n","# Definiendo los filtros.\n","filter_1 = filter_vals\n","filter_2 = -filter_1\n","filter_3 = filter_1.T\n","filter_4 = -filter_3\n","# Añadimos una más para demostrar los resultados de un filtro distinto\n","filter_5 = np.array([[-1,   5,   5,   5],\n","                     [-1,  -1,   5,   5],\n","                     [-1,  -1,  -1,   5],\n","                     [-1,  -1,  -1,  -1]])\n","filters = np.array([filter_1, filter_2, filter_3, filter_4, filter_5])\n","\n","# Miremos uno.\n","print('Filter 1: \\n', filter_1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sk77gJ_vgCvM"},"source":["# Veamos todos los filtros.\n","fig = plt.figure(figsize=(10, 5))\n","for i in range(5):\n","    ax = fig.add_subplot(1, 5, i+1, xticks=[], yticks=[])\n","    ax.imshow(filters[i], cmap='gray')\n","    ax.set_title('Filter %s' % str(i+1))\n","    width, height = filters[i].shape\n","    for x in range(width):\n","        for y in range(height):\n","            ax.annotate(str(filters[i][x][y]), xy=(y,x),\n","                        horizontalalignment='center',\n","                        verticalalignment='center',\n","                        color='white' if filters[i][x][y]<0 else 'black')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R6iZBMHLgCvM"},"source":["### Definamos una capa convolucional.\n","\n","\n","Partimos definiendo una capa Convolucional. Inicialice una sola capa convolucional para que contenga todos los filtros creados. Tenga en cuenta que no está entrenando esta red; ¡Está inicializando los pesos en una capa convolucional para que pueda visualizar lo que sucede después de un paso directo a través de esta red!\n","\n","\n","#### `__init__` and `forward`\n","\n","Para definir una red neuronal en PyTorch, defina las capas de un modelo en la función `__init__` y defina el comportamiento hacia adelante de una red que aplica esas capas inicializadas a una entrada (` x`) en la función `forward`. En PyTorch, convertimos todas las entradas en el tipo de datos Tensor.\n","\n","A continuación, se define la estructura de una clase llamada `Net` que tiene una capa convolucional que puede contener cuatro filtros de escala de grises 4x4."]},{"cell_type":"markdown","metadata":{"id":"EjIKW9v5hM9_"},"source":["## <font color='green'>**Actividad 1**</font>\n","\n","En esta actividad queremos visualizar la salida de una capa convolucional. Para esto vamos a definir una capa convolucional y le vamos a asignar pesos de la siguiente forma:\n","\n","\n","```python\n","k_height, k_width = weight.shape[2:]\n","# Al crear la capa se indica que son 4 filtros.\n","self.conv = nn.Conv2d(1, 4, kernel_size=(k_height, k_width), bias=False)\n","self.conv.weight = torch.nn.Parameter(weight) # Estamos asignando los pesos.\n","```\n","\n","Luego realice al forward y devuelva  el resultado de aplicar la capa convolucional y y la capa aplicando la activación con relu.\n","\n","```python\n","    def forward(self, x):\n","        # Calcula la salida de la capa convolucional.\n","        conv_x = self.conv(x) #Aqui aplica los kernel.\n","        activated_x = F.relu(conv_x)# Aqui aplica una relu.\n","        \n","        # retornamos ambas capas.\n","        return conv_x, activated_x\n","    \n","```\n","\n","Finalmente asigne los pesos de la siguiente forma.\n","\n","\n","\n","```python\n","weight = torch.from_numpy(filters).unsqueeze(1).type(torch.FloatTensor)\n","model = Net(weight)\n","\n","```\n","\n","\n","\n"]},{"cell_type":"markdown","source":["<font color='green'>Fin actividad 1</font>"],"metadata":{"id":"Kc7nqnqg4VnY"}},{"cell_type":"markdown","metadata":{"id":"FoqBvCtmgCvO"},"source":["### Vamos ahora a visualizar la salida de una capa.\n","\n","Primero, definiremos una función auxiliar, `viz_layer` que toma una capa específica y una cantidad de filtros (argumento opcional), y muestra la salida de esa capa una vez que se ha pasado una imagen. Utilice el siguiente código.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"qHed09bDgCvO"},"source":["# Funcion que permite visualizar la salida de una capa.\n","# Por defecto para 4 filtros.\n","def viz_layer(layer, n_filters= 5):\n","    fig = plt.figure(figsize=(20, 20))\n","\n","    for i in range(n_filters):\n","        ax = fig.add_subplot(1, n_filters, i+1, xticks=[], yticks=[])\n","        # Guardar layer outputs\n","        ax.imshow(np.squeeze(layer[0,i].data.numpy()), cmap='gray') # LA transformamos al formato adecuado para visualizar.\n","        ax.set_title('Salida %s' % str(i+1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d_-KwW-WgCvQ"},"source":["# Imagen original.\n","plt.imshow(gray_img, cmap='gray')\n","\n","# Visualizando los filtros.\n","fig = plt.figure(figsize=(12, 8))\n","fig.subplots_adjust(left=0, right=1.5, bottom=0.8, top=1, hspace=0.05, wspace=0.05)\n","for i in range(5):\n","    ax = fig.add_subplot(1, 5, i+1, xticks=[], yticks=[])\n","    ax.imshow(filters[i], cmap='gray')\n","    ax.set_title('Filter %s' % str(i+1))\n","\n","\n","# Convirtiendo a un tensor la imagen para ser procesada por la red.\n","gray_img_tensor = torch.from_numpy(gray_img).unsqueeze(0).unsqueeze(1)\n","\n","# Obtengamos las capas.\n","conv_layer, activated_layer = model(gray_img_tensor)\n","\n","# Veamos la salida. SIn aplicar relu\n","viz_layer(conv_layer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# (batch_size, channels, height, width)\n","gray_img_tensor.size()"],"metadata":{"id":"Dit1UL3f3ZIl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xc_KoSfngCvR"},"source":["### ReLu activation\n","\n","En este modelo, hemos utilizado una función de activación que escala la salida de la capa convolucional. Hemos elegido una función ReLu para hacer esto, y esta función simplemente convierte todos los valores de píxeles negativos en ceros (negro). Consulte la ecuación que se muestra a continuación para ver los valores de píxeles de entrada, `x`.\n","\n","<img alt=\"ReLu activation\"  src=\"https://drive.google.com/uc?export=view&id=1gWJkNE9ohn0mR6EbKTCNNvIECt7Ko8Hl\" width=600px>\n"]},{"cell_type":"code","metadata":{"id":"iadKpghcgCvS"},"source":["# Aplicando Relu\n","viz_layer(activated_layer)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?export=view&id=1DNuGbS1i-9it4Nyr3ZMncQz9cRhs2eJr\" width=\"100\" align=\"left\" title=\"Runa-perth\">\n","<br clear=\"left\">\n","\n","## <font color='green'>**La inspiracion del proceso de vision computacional**</font>\n","\n","El trabajo de David Hubel y Torsten Wiesel en los años 50 y 60 del siglo pasado sentó las bases para nuestra comprensión de la fisiología visual y, eventualmente, inspiró avances en el campo de la visión artificial y el aprendizaje profundo. Sus experimentos, realizados principalmente en gatos, revolucionaron nuestra comprensión del procesamiento visual en el cerebro.\n","\n","\n","1. Experimentos de Hubel y Wiesel:\n","\n","  a. Hubel y Wiesel introdujeron microelectrodos en la corteza visual de gatos para registrar la actividad de las neuronas individuales en respuesta a estímulos visuales específicos.\n","\n","  b. Descubrieron células simples en la corteza visual primaria que respondían mejor a barras de luz orientadas en un ángulo particular. Estas células no responden tan bien si el ángulo de la barra cambia.\n","\n","  c. También encontraron células complejas que también respondían a barras orientadas, pero eran más robustas con respecto a la posición exacta de la barra en el campo visual.\n","\n","  d. Estos hallazgos sugirieron una jerarquía en el procesamiento visual: la información se procesa en etapas, con cada etapa agregando un nivel de complejidad al procesamiento.\n","\n","2. Inspiración para Redes Neuronales:\n","\n","  a. La idea de que el cerebro tiene neuronas especializadas para detectar características específicas se convirtió en la base para la idea de \"detectores de características\" en la visión artificial.\n","\n","  b. Las células simples y complejas pueden considerarse como análogos a los filtros en las capas convolucionales de una red neuronal, que se activan en respuesta a características específicas (por ejemplo, bordes orientados en un ángulo particular).\n","\n","3. Capas Profundas en Visión Artificial:\n","\n","  a. Siguiendo la idea de un proceso jerárquico en el cerebro, las redes neuronales profundas (Deep Neural Networks, DNNs) se construyen con muchas capas, donde cada capa procesa características de nivel superior basadas en la salida de la capa anterior.\n","\n","  b. En el contexto de la visión artificial, las primeras capas de una red neuronal convolucional (CNN) podrían aprender detectores de bordes (similar a las células simples), mientras que las capas más profundas podrían aprender a detectar formas complejas o incluso objetos completos (análogo a estructuras cerebrales más avanzadas que procesan información visual).\n","\n","  c. Así, las redes neuronales profundas intentan imitar este proceso jerárquico, permitiendo que la red aprenda automáticamente características de nivel superior a partir de datos sin procesar."],"metadata":{"id":"tIIpzNj5PgZk"}},{"cell_type":"markdown","source":["### Beneficios del Max Pooling:\n","\n","1. Reducción de Dimensionalidad: El Max Pooling reduce significativamente la cantidad de parámetros y cálculos en la red. Esto no solo ayuda a prevenir el sobreajuste sino que también acelera el entrenamiento.\n","\n","2. Invariancia Espacial: El Max Pooling introduce cierta invariancia a la traslación. Si una característica es detectada en alguna parte de la imagen, su ubicación exacta se vuelve menos relevante, lo que permite a la CNN reconocer objetos independientemente de su posición exacta en la imagen.\n","\n","3. Conservación de Características Importantes: Al tomar el valor máximo de una ventana, se está conservando la característica más prominente en esa región y descartando información no esencial."],"metadata":{"id":"fnRD4Z5SQtQs"}},{"cell_type":"markdown","source":["## <font color='green'>**Actividad: Aplicar la operacion de convolucion a distintas imagenes y distintos filtros**</font>\n","\n","\n","Aplique filtros convolucionales a una imagen en escala de grises para ver cómo funciona la convolución.\n","\n","\n","### Filtro de deteccion de bordes\n","```\n","[[1, 0, -1],\n"," [1, 0, -1],\n"," [1, 0, -1]]\n","```\n","\n","### Filtro blur\n","```\n","[[1/9, 1/9, 1/9],\n"," [1/9, 1/9, 1/9],\n"," [1/9, 1/9, 1/9]]\n","```\n","\n","### Filtro Gaussiano\n","\n","```\n","[[1, 2, 1],\n"," [2, 4, 2],\n"," [1, 2, 1]]\n","```\n","\n","### Filtro Emboss\n","```\n","[[-2, -1, 0],\n"," [-1, 1, 1],\n"," [0, 1, 2]]\n","```\n","\n","### Bordes en 45 grados\n","```\n","[[-1, 1, 1],\n"," [-1, -2, 1],\n"," [-1, 1, 1]]\n","```\n","\n","### Bordes en 135 grados\n","```\n","[[1, 1, -1],\n"," [1, -2, -1],\n"," [1, 1, -1]]\n","```\n","\n","### Bordes\n","```\n","\n","[   [1, 2, 0, -2, -1],\n","    [1, 2, 0, -2, -1],\n","    [1, 2, 0, -2, -1],\n","    [1, 2, 0, -2, -1],\n","    [1, 2, 0, -2, -1]\n","]\n","```\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"0z267aTp4ZfE"}},{"cell_type":"markdown","source":["## <font color='green'>**Actividad: Aplicación de diferentes funciones de activación tras convolución**</font>\n","\n","Objetivo: Familiarizarse con el efecto de diferentes funciones de activación después de aplicar convolución en imágenes.\n","\n","Utilizando PyTorch, carga una imagen de tu elección (preferentemente una en la que se puedan distinguir detalles y contrastes).\n","\n","Define un filtro para la detección de bordes (puedes usar el filtro Sobel o cualquier otro que prefieras).\n","\n","**Actividades:**\n","a. Aplica la convolución de la imagen con el filtro definido.\n","\n","b. Aplica la siguiente lista de funciones de activación al resultado de la convolución:\n","\n","  - ReLU\n","  - Sigmoid\n","  - Tanh\n","  - Leaky ReLU (puedes experimentar con diferentes valores para el parámetro de inclinación)\n","\n","c. Visualiza la imagen original, el resultado de la convolución y el resultado tras aplicar cada función de activación.\n","\n","**Preguntas:**\n","\n","- ¿Qué efecto tiene cada función de activación en la imagen resultante?\n","- ¿Hay alguna función de activación que resalte mejor ciertas características de la imagen original después de la convolución? Si es así, ¿cuál y por qué?\n","- ¿Cómo cambiaría el resultado si se utiliza un filtro diferente?\n","\n","Repite el proceso con diferentes filtros (por ejemplo, un filtro de realce o un filtro gaussiano) y compara los efectos de las funciones de activación en los resultados."],"metadata":{"id":"nRKr_s9pmXeI"}},{"cell_type":"markdown","source":["## <font color='green'>**Actividad: Aprendizaje de un filtro convolucional para detección de bordes**</font>\n","\n","Objetivo: Entender cómo un filtro convolucional puede aprender a detectar características en imágenes a través del entrenamiento.\n","\n","\n","**Carga una imagen de tu elección en PyTorch.**\n","\n","Utiliza un filtro Sobel (o cualquier otro filtro de detección de bordes) para generar una imagen \"objetivo\" con los bordes detectados.\n","\n","Define un filtro convolucional de la misma dimensión que el filtro Sobel, pero inicialízalo con valores aleatorios. Este será el filtro que se entrenará.\n","\n","**Proceso de aprendizaje:**\n","\n","- Aplica el filtro convolucional (con valores aleatorios) a la imagen original para obtener una imagen \"predicha\".\n","\n","- Calcula la pérdida como la diferencia al cuadrado entre la imagen \"objetivo\" (Sobel) y la imagen \"predicha\".\n","\n","- Usa backpropagation y un optimizador (como SGD) para actualizar los valores del filtro convolucional y reducir la pérdida.\n","\n","- Repite este proceso durante varias iteraciones (epochs) hasta que la pérdida converja a un valor mínimo.\n","\n","**Visualización:**\n","\n","- Muestra la imagen original, la imagen \"objetivo\" (Sobel) y la imagen \"predicha\" después del entrenamiento.\n","\n","- Compara visualmente la imagen \"objetivo\" con la imagen \"predicha\" para evaluar qué tan bien aprendió el filtro a detectar bordes.\n","\n","**Preguntas:**\n","\n","- ¿Cómo se compara la imagen \"predicha\" con la imagen \"objetivo\"?\n","- ¿Qué ocurre si se cambia la imagen original por otra diferente y se aplica el filtro entrenado? ¿Detecta correctamente los bordes?\n","- ¿Qué sucede si se inicializa el filtro con diferentes valores aleatorios y se repite el proceso de entrenamiento?\n","\n","**Adicional**\n","\n","Experimenta entrenando el filtro con diferentes imágenes y compara los resultados. ¿Puede un filtro aprender a detectar bordes de manera generalizada en diferentes imágenes?"],"metadata":{"id":"gtbler4PoZpq"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","import cv2\n","\n","# Cargar imagen en escala de grises\n","img = cv2.imread('/content/drive/MyDrive/Curso/Industria Inteligente/2023-2S/Datos/Auto.jfif', cv2.IMREAD_GRAYSCALE)\n","plt.imshow(img, cmap='gray')\n","plt.title(\"Imagen Original\")\n","plt.show()\n","\n","# Convertir la imagen a tensor y añadir dimensiones adicionales\n","img_tensor = torch.from_numpy(img).float().unsqueeze(0).unsqueeze(1)\n","\n","# Definir un filtro de detección de bordes\n","#filter_edge = torch.Tensor([[1, 0, -1],\n","#                            [1, 0, -1],\n","#                            [1, 0, -1]])\n","\n","#filter_edge = torch.Tensor([[-1, 1, 1],\n","#                            [-1, -2, 1],\n","#                            [-1, 1, 1]])\n","\n","filter_edge = torch.Tensor([[1/36, 1/36, 1/36, 1/36, 1/36, 1/36],\n","                            [1/36, 1/36, 1/36, 1/36, 1/36, 1/36],\n","                            [1/36, 1/36, 1/36, 1/36, 1/36, 1/36],\n","                            [1/36, 1/36, 1/36, 1/36, 1/36, 1/36],\n","                            [1/36, 1/36, 1/36, 1/36, 1/36, 1/36],\n","                            [1/36, 1/36, 1/36, 1/36, 1/36, 1/36]])\n","\n","conv = nn.Conv2d(1, 1, kernel_size=(6, 6), bias=False)\n","conv.weight.data = filter_edge.unsqueeze(0).unsqueeze(1)\n","\n","# Aplicar el filtro\n","output = conv(img_tensor)\n","\n","# Mostrar el resultado\n","plt.imshow(output[0, 0].detach().numpy(), cmap='gray')\n","plt.title(\"Detección de Bordes\")\n","plt.show()\n"],"metadata":{"id":"e83kKG9z4aBC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","# Cargar la imagen y convertirla a tensor\n","transform = transforms.Compose([\n","    transforms.Grayscale(),  # Convertir a escala de grises para simplificar\n","    transforms.ToTensor()\n","])\n","img_path = '/content/drive/MyDrive/Curso/Industria Inteligente/2023-2S/Datos/Auto.jfif'\n","image = Image.open(img_path)\n","tensor_image = transform(image).unsqueeze(0)  # Añadir una dimensión para el batch\n","\n","# Definir un filtro (por ejemplo, un filtro Sobel para detección de bordes)\n","sobel_filter = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32)\n","sobel_filter = sobel_filter.unsqueeze(0).unsqueeze(0)\n","\n","# Aplicar convolución\n","output = F.conv2d(tensor_image, sobel_filter, padding=1)\n","\n","# Aplicar función de activación (ejemplo: ReLU)\n","activated_output = F.relu(output)\n","\n","# Visualizar resultados\n","def tensor_to_image(tensor):\n","    # Convertir el tensor a una imagen de PIL para visualizar\n","    return transforms.ToPILImage()(tensor.squeeze())\n","\n","# Imagen original\n","plt.subplot(1, 3, 1)\n","plt.imshow(image, cmap='gray')\n","plt.title(\"Original\")\n","\n","# Resultado de la convolución\n","plt.subplot(1, 3, 2)\n","plt.imshow(tensor_to_image(output), cmap='gray')\n","plt.title(\"Convolución\")\n","\n","# Resultado después de la función de activación\n","plt.subplot(1, 3, 3)\n","plt.imshow(tensor_to_image(activated_output), cmap='gray')\n","plt.title(\"Activación\")\n","\n","plt.show()\n"],"metadata":{"id":"U092G_GXlgeA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","# Cargar imagen y convertirla a tensor\n","transform = transforms.Compose([\n","    transforms.Grayscale(),\n","    transforms.ToTensor()\n","])\n","img_path = '/content/drive/MyDrive/Curso/Industria Inteligente/2023-2S/Datos/Auto.jfif'\n","image = Image.open(img_path)\n","tensor_image = transform(image).unsqueeze(0)  # Añadir dimensión para el batch\n","\n","# Generar imagen objetivo usando filtro Sobel\n","sobel_filter = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32)\n","sobel_filter = sobel_filter.unsqueeze(0).unsqueeze(0)\n","target = nn.functional.conv2d(tensor_image, sobel_filter, padding=1)\n","\n","# Definir filtro convolucional a entrenar, inicializado aleatoriamente\n","learning_filter = nn.Parameter(torch.randn_like(sobel_filter))\n","\n","# Definir optimizador\n","optimizer = optim.SGD([learning_filter], lr=0.01)\n","\n","# Función de pérdida\n","loss_fn = nn.MSELoss()\n","\n","# Entrenar el filtro\n","num_epochs = 5000\n","for epoch in range(num_epochs):\n","    optimizer.zero_grad()\n","    output = nn.functional.conv2d(tensor_image, learning_filter, padding=1)\n","    loss = loss_fn(output, target)\n","    loss.backward()\n","    optimizer.step()\n","    if epoch % 100 == 0:\n","        print(f\"Epoch {epoch}/{num_epochs}, Loss: {loss.item()}\")\n","\n","# Visualizar resultados\n","def tensor_to_image(tensor):\n","    return transforms.ToPILImage()(tensor.squeeze())\n","\n","plt.subplot(1, 3, 1)\n","plt.imshow(image, cmap='gray')\n","plt.title(\"Original\")\n","\n","plt.subplot(1, 3, 2)\n","plt.imshow(tensor_to_image(target), cmap='gray')\n","plt.title(\"Objetivo (Sobel)\")\n","\n","plt.subplot(1, 3, 3)\n","plt.imshow(tensor_to_image(output.detach()), cmap='gray')\n","plt.title(\"Predicción\")\n","\n","plt.show()\n"],"metadata":{"id":"xUEmfSkznX5a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output.shape"],"metadata":{"id":"t2X0xBU8xYiM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Kernel entrenado:\")\n","print(learning_filter)\n","\n","\n","[[-1, 0, 1],\n"," [-2, 0, 2],\n","  [-1, 0, 1]]"],"metadata":{"id":"t0Aa9XMPseM9"},"execution_count":null,"outputs":[]}]}