{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hCEsCVdjU_I9"},"source":["# **Aprendizaje supervisado**\n","# SL06. Naive Bayes"]},{"cell_type":"markdown","metadata":{"id":"Iv7EoTE7U_I_"},"source":["Los modelos Naive Bayes son un grupo de algoritmos de clasificación extremadamente rápidos y simples que a menudo son adecuados para conjuntos de datos de muy alta dimensión.\n","Debido a que son tan rápidos y tienen tan pocos parámetros ajustables, terminan siendo muy útiles como un modelo base rápido para un problema de clasificación.\n","Esta sección se centrará en una explicación intuitiva de cómo funcionan los clasificadores de Bayes ingenuos, seguida de un par de ejemplos de ellos en acción en algunos conjuntos de datos."]},{"cell_type":"markdown","metadata":{"id":"w21Wj3ujU_I_"},"source":["## <font color='blue'>**Clasificación Bayesiana**</font>\n","\n","Los **clasificadores Naive Bayes** se basan en métodos de clasificación Bayesianos.\n","Estos se basan en el teorema de Bayes, que es una ecuación que describe la relación de probabilidades condicionales de cantidades estadísticas.\n","En la clasificación bayesiana, estamos interesados ​​en encontrar la probabilidad de una etiqueta dadas algunas características observadas, que podemos escribir como $ P (L ~ | ~ {\\rm características}) $.\n","El teorema de Bayes nos dice cómo expresar esto en términos de cantidades que podemos calcular más directamente:\n","\n","$$\n","P (L ~ | ~ {\\rm características}) = \\frac {P ({\\rm características} ~ | ~ L) P (L)} {P ({\\rm características})}\n","$$\n","\n","Si estamos tratando de decidir entre dos etiquetas, llamémoslas $ L_1 $ y $ L_2 $, entonces una forma de tomar esta decisión es calcular la razón de las probabilidades posteriores para cada etiqueta:\n","\n","$$\n","\\frac {P (L_1 ~ | ~ {\\rm características})} {P (L_2 ~ | ~ {\\rm características})} = \\frac {P ({\\rm características} ~ | ~ L_1)} {P ( {\\rm características} ~ | ~ L_2)} \\frac {P (L_1)} {P (L_2)}\n","$$\n","\n","Todo lo que necesitamos ahora es algún modelo mediante el cual podamos calcular $ P ({\\rm features} ~ | ~ L_i) $ para cada etiqueta.\n","Este modelo se denomina *modelo generativo* porque especifica el proceso aleatorio hipotético que genera los datos.\n","Especificar este modelo generativo para cada etiqueta es la pieza principal del entrenamiento de dicho clasificador bayesiano.\n","La versión general de este paso de formación es una tarea muy difícil, pero podemos simplificarla mediante el uso de algunas suposiciones simplificadoras sobre la forma de este modelo.\n","\n","Aquí es donde entra el \"ingenuo\" en \"Bayes ingenuo\": si hacemos suposiciones muy ingenuas sobre el modelo generativo para cada etiqueta, podemos encontrar una aproximación aproximada del modelo generativo para cada clase, y luego proceder con la clasificación bayesiana .\n","Los diferentes tipos de clasificadores de Bayes ingenuos se basan en diferentes supuestos ingenuos sobre los datos, y examinaremos algunos de ellos en las siguientes secciones.\n","\n","Comenzamos con las importaciones estándar:"]},{"cell_type":"code","metadata":{"id":"l-1xFYxxU_JB"},"source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns; sns.set()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wSjo0hVxU_JK"},"source":["## <font color='blue'>**Gaussian Naive Bayes**</font>\n","\n","Quizás el clasificador de Bayes ingenuo más fácil de entender es el Bayes ingenuo de Gauss.\n","En este clasificador, se supone que *los datos de cada etiqueta se extraen de una distribución gaussiana simple*.\n","Imagina que tienes los siguientes datos:"]},{"cell_type":"code","metadata":{"id":"w-Q71xmYU_JL"},"source":["from sklearn.datasets import make_blobs\n","X, y = make_blobs(100, 2, centers=2, random_state=2, cluster_std=1.5)\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XKOWhkBRU_JR"},"source":["Una forma extremadamente rápida de crear un modelo simple es asumir que los datos se describen mediante una distribución gaussiana sin covarianza entre dimensiones.\n","Este modelo se puede ajustar simplemente encontrando la desviación estándar y media de los puntos dentro de cada etiqueta, que es todo lo que necesita para definir dicha distribución.\n","El resultado de esta ingenua suposición gaussiana se muestra en la siguiente figura:"]},{"cell_type":"markdown","metadata":{"id":"o56T8RiHU_JT"},"source":["![(run code in Appendix to generate image)](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.05-gaussian-NB.png?raw=1)\n"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"LwAy1Xv7U_JT"},"source":["Las elipses aquí representan el modelo generativo gaussiano para cada etiqueta, con mayor probabilidad hacia el centro de las elipses.\n","Con este modelo generativo implementado para cada clase, tenemos una receta simple para calcular la probabilidad $ P ({\\rm features} ~ | ~ L_1) $ para cualquier punto de datos, y así podemos calcular rápidamente la razón posterior y determinar qué etiqueta es la más probable para un punto dado.\n","\n","Este procedimiento se implementa en el estimador ''sklearn.naive_bayes.GaussianNB'' de Scikit-Learn:"]},{"cell_type":"code","metadata":{"id":"2ScC7IsdU_JU"},"source":["from sklearn.naive_bayes import GaussianNB\n","model = GaussianNB()\n","model.fit(X, y);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wS5KI-3NU_Ja"},"source":["Ahora generemos algunos datos nuevos y hagamos una prediccion de la etiqueta:"]},{"cell_type":"code","metadata":{"id":"nOuFT4n4U_Jb"},"source":["rng = np.random.RandomState(0)\n","Xnew = [-6, -14] + [14, 18] * rng.rand(2000, 2)\n","ynew = model.predict(Xnew)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MYEGQCFEU_Jg"},"source":["Ahora podemos trazar estos nuevos datos para tener una idea de dónde está el límite de decisión:"]},{"cell_type":"code","metadata":{"id":"_IABXtSxU_Jh"},"source":["plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')\n","lim = plt.axis()\n","plt.scatter(Xnew[:, 0], Xnew[:, 1], c=ynew, s=20, cmap='RdBu', alpha=0.1)\n","plt.axis(lim);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CLYJyTp-U_Jl"},"source":["Vemos un límite ligeramente curvado en las clasificaciones; en general, el límite en el Bayes ingenuo de Gauss es cuadrático.\n","\n","Una buena parte de este formalismo bayesiano es que, naturalmente, permite la clasificación probabilística, que podemos calcular utilizando el método ''predict_proba'':"]},{"cell_type":"code","metadata":{"id":"LpPKtRiEU_Jm"},"source":["yprob = model.predict_proba(Xnew)\n","yprob[-8:].round(2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2_gvvC_yU_Jr"},"source":["Las columnas dan las probabilidades posteriores de la primera y segunda etiqueta, respectivamente.\n","Si está buscando estimaciones de incertidumbre en su clasificación, los enfoques bayesianos como este pueden ser un enfoque útil.\n","\n","Por supuesto, la clasificación final solo será tan buena como los supuestos del modelo que la conducen, razón por la cual el Bayes ingenuo gaussiano a menudo no produce muy buenos resultados.\n","Aún así, en muchos casos, especialmente a medida que aumenta el número de características, esta suposición no es lo suficientemente perjudicial como para evitar que el Bayes ingenuo gaussiano sea un método útil."]},{"cell_type":"markdown","metadata":{"id":"tOcWmsDjU_Js"},"source":["## <font color='blue'>**Multinomial Naive Bayes**</font>\n","\n","La suposición gaussiana que se acaba de describir no es de ninguna manera la única suposición simple que podría usarse para especificar la distribución generativa para cada etiqueta.\n","Otro ejemplo útil es el Bayes ingenuo multinomial, donde se supone que las características se generan a partir de una distribución multinomial simple.\n","La distribución multinomial describe la probabilidad de observar recuentos entre varias categorías y, por lo tanto, Bayes ingenuo multinomial es más apropiado para características que representan recuentos o tasas de recuento.\n","\n","La idea es precisamente la misma que antes, excepto que en lugar de modelar la distribución de datos con el mejor ajuste gaussiano, modelamos la distribución de datos con una distribución multinomial de mejor ajuste."]},{"cell_type":"markdown","metadata":{"id":"8ZxHSMKJU_Js"},"source":["### Ejemplo: Clasificacion de Texto\n","\n","Un lugar donde a menudo se usa Bayes ingenuo multinomial es en la clasificación de texto, donde las características están relacionadas con el recuento de palabras o las frecuencias dentro de los documentos que se van a clasificar.\n","Aquí usaremos las funciones de recuento de palabras dispersas del corpus de 20 grupos de noticias para mostrar cómo podemos clasificar estos documentos breves en categorías.\n","\n","Descarguemos los datos y echemos un vistazo a los nombres de los objetivos:"]},{"cell_type":"code","metadata":{"id":"T8C7Ji_mU_Jt"},"source":["from sklearn.datasets import fetch_20newsgroups\n","\n","data = fetch_20newsgroups()\n","data.target_names"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z_XRNzIWU_Jy"},"source":["Para simplificar aquí, seleccionaremos solo algunas de estas categorías y descargaremos el conjunto de entrenamiento y prueba:"]},{"cell_type":"code","metadata":{"id":"QePBK82QU_Jy"},"source":["categories = ['talk.religion.misc', 'soc.religion.christian',\n","              'sci.space', 'comp.graphics']\n","train = fetch_20newsgroups(subset='train', categories=categories)\n","test = fetch_20newsgroups(subset='test', categories=categories)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dY0xt03-U_J3"},"source":["Aquí hay una entrada representativa de los datos:"]},{"cell_type":"code","metadata":{"id":"753T_zLXU_J3"},"source":["print(train.data[5])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lFdQTwZdU_J8"},"source":["Para utilizar estos datos para el aprendizaje automático, necesitamos poder convertir el contenido de cada cadena en un vector de números.\n","Para esto usaremos el vectorizador TF-IDF, y crearemos un Pipeline que lo adjunte a un clasificador Bayes ingenuo multinomial:"]},{"cell_type":"code","metadata":{"id":"wmg7NALbU_J9"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(TfidfVectorizer(), MultinomialNB())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6VlkUpwlU_KB"},"source":["Con este Pipeline, podemos aplicar el modelo a los datos de entrenamiento y predecir etiquetas para los datos de prueba:"]},{"cell_type":"code","metadata":{"id":"cMMQ7WnVU_KC"},"source":["model.fit(train.data, train.target)\n","labels = model.predict(test.data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W9-9I93tU_KG"},"source":["Ahora que hemos predicho las etiquetas de los datos de prueba, podemos evaluarlas para conocer el rendimiento del estimador.\n","Por ejemplo, aquí está la matriz de confusión entre las etiquetas verdaderas y predichas para los datos de prueba:"]},{"cell_type":"code","metadata":{"id":"hWs_lCr9U_KH"},"source":["from sklearn.metrics import confusion_matrix\n","mat = confusion_matrix(test.target, labels)\n","sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n","            xticklabels=train.target_names, yticklabels=train.target_names)\n","plt.xlabel('true label')\n","plt.ylabel('predicted label');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gaXoXaA6U_KK"},"source":["Evidentemente, incluso este clasificador muy simple puede separar con éxito la conversación sobre espacio de la conversación sobre computadoras, pero se confunde entre hablar sobre religión y hablar sobre cristianismo.\n","¡Esta es quizás un área de confusión esperada!\n","\n","Lo bueno aquí es que ahora tenemos las herramientas para determinar la categoría de * cualquier * cadena, usando el método `` predict () '' de este Pipeline.\n","Aquí hay una función de utilidad rápida que devolverá la predicción para una sola cadena:"]},{"cell_type":"code","metadata":{"id":"_bMt_RwdU_KM"},"source":["def predict_category(s, train=train, model=model):\n","    pred = model.predict([s])\n","    return train.target_names[pred[0]]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I5ijuu-vU_KQ"},"source":["Probemos:"]},{"cell_type":"code","metadata":{"id":"9DcyNKRlU_KR"},"source":["predict_category('sending a payload to the ISS')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iFWM7KDsU_KU"},"source":["predict_category('discussing islam vs atheism')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gHSXEX6KU_KX"},"source":["predict_category('determining the screen resolution')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2YXNRkG4U_Ka"},"source":["Recuerde que esto no es nada más sofisticado que un modelo de probabilidad simple para la frecuencia (ponderada) de cada palabra en la cadena; sin embargo, el resultado es sorprendente.\n","Incluso un algoritmo muy ingenuo, cuando se usa con cuidado y se entrena en un gran conjunto de datos de alta dimensión, puede ser sorprendentemente efectivo."]},{"cell_type":"markdown","metadata":{"id":"ZLVM3HEbU_Kb"},"source":["### ¿Cuándo usar Naive Bayes?\n","\n","Debido a que los clasificadores bayesianos ingenuos hacen suposiciones tan estrictas sobre los datos, generalmente no funcionarán tan bien como un modelo más complicado.\n","Dicho esto, tienen varias ventajas:\n","\n","- Son extremadamente rápidos tanto para el entrenamiento como para la predicción.\n","- Proporcionan una predicción probabilística sencilla\n","- A menudo son muy fáciles de interpretar.\n","- Tienen muy pocos (si los hay) parámetros ajustables\n","\n","Estas ventajas significan que un clasificador bayesiano ingenuo suele ser una buena opción como clasificación inicial de referencia.\n","Si funciona adecuadamente, felicitaciones: tiene un clasificador muy rápido y muy interpretable para su problema.\n","Si no funciona bien, puede comenzar a explorar modelos más sofisticados, con un conocimiento básico de qué tan bien deberían funcionar.\n","\n","Los clasificadores Naive Bayes tienden a funcionar especialmente bien en una de las siguientes situaciones:\n","\n","- Cuando las suposiciones ingenuas realmente coinciden con los datos (muy raro en la práctica)\n","- Para categorías muy bien separadas, cuando la complejidad del modelo es menos importante\n","- Para datos de muy alta dimensión, cuando la complejidad del modelo es menos importante\n","\n","Los dos últimos puntos parecen distintos, pero en realidad están relacionados: a medida que crece la dimensión de un conjunto de datos, es mucho menos probable que dos puntos se encuentren juntos (después de todo, deben estar cerca en *cada una de las dimensiones* para estar cerca en general).\n","Esto significa que los conglomerados en dimensiones altas tienden a estar más separados, en promedio, que los conglomerados en dimensiones bajas, asumiendo que las nuevas dimensiones realmente agregan información.\n","Por esta razón, los clasificadores simplistas como los ingenuos Bayes tienden a funcionar tan bien o mejor que los clasificadores más complicados a medida que aumenta la dimensionalidad: una vez que tiene suficientes datos, incluso un modelo simple puede ser muy poderoso."]},{"cell_type":"markdown","metadata":{"id":"RtIp_R_CnQIH"},"source":["## <font color='green'>Actividad 1</font>\n","\n","Construiremos clasificadores utilizando modelos Bayesianos para el dataset de la flor iris."]},{"cell_type":"code","metadata":{"id":"NnFSjUEenQII"},"source":["from sklearn.datasets import load_iris\n","data = load_iris()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uv8TE3sEnQII"},"source":["# Tu código aquí\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LNcbhWaxpALf"},"source":["<font color='green'>Fin Actividad 1</font>"]}]}