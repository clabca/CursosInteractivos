{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"DBjjYdCD7uQV"},"source":["# **Aprendizaje supervisado**\n","# SL12. Gradient boosting"]},{"cell_type":"markdown","metadata":{"id":"A2sLrEarXveT"},"source":["## <font color='blue'>**Métodos Combinados de Aprendizaje (Emsemble)**</font>\n","\n","Los métodos combinados (métodos de ensemble) utilizan múltiples algoritmos de aprendizaje para obtener un rendimiento predictivo que mejore el que podría obtenerse por medio de cualquiera de los algoritmos de aprendizaje individuales que lo constituyen.\n","\n","<img src='https://drive.google.com/uc?export=view&id=17NcrKxig4ihOpnwxbRSYdQgejSRvHbTh' width=\"600\" align=\"center\" style=\"margin-right: 20px\">\n","\n","La idea de los métodos ensemble es considerar múltiples hipótesis simultáneamente para formar una hipótesis que, con suerte (y la ayuda de algunos teoremas esenciales), se comporte mejor. El término de métodos de ensemble se suele reservar para aquellas combinaciones que hacen uso de múltiples hipótesis pertenecientes a una misma familia, mientras que se usa el término más general de sistemas de aprendizaje múltiples cuando las hipótesis que se combinan provienen de diversas familias.\n","\n","Evidentemente, debido a que los métodos combinados hacen uso de varias hipótesis simultáneas, se produce una elevación en los costos computacionales, por lo que suele ser habitual utilizar algoritmos rápidos como espacio de hipótesis base, como son los árboles de decisión.\n","\n","Una combinación de algoritmos de aprendizaje supervisado es en si mismo un algoritmo de aprendizaje supervisado y puede ser entrenado y usado para hacer predicciones. Sin embargo, se debe tener en cuenta que una combinación de hipótesis (algoritmos) de una determinada familia no es necesariamente una hipótesis (algoritmo) de la misma familia, por lo que podríamos obtener mejores resultados que con los elementos individuales de la familia, aunque también podemos correr el riesgo de obtener un modelo sobreajustado si no se tienen algunas precauciones. En la práctica, la forma en que se seleccionan los modelos individuales que se combinan hacen uso de algunas técnicas que tienden a reducir los problemas relacionados con el exceso de ajuste de los datos de entrenamiento y mejoran la prediccióm conjunta.\n","\n","**Empíricamente, se ha comprobado que cuando existe una diversidad significativa entre los modelos individuales, las combinaciones tienden a obtener mejores resultados**, por lo que muchos de los métodos existentes buscan promover la diversidad entre los modelos que se combinan, y ello provoca a veces que se usen como modelos aquellos que hacen un uso fuerte de la aleatoriedad, en vez de modelos más dirigidos y que funcionan mejor individualmente."]},{"cell_type":"markdown","metadata":{"id":"TwvJj8OtZGvn"},"source":["## <font color='blue'>**Agregación Bootstrap (Bagging)**</font>\n","\n","La **agregación Bootstrap**, tambien conocida por **Bagging**, es realmente un meta-algoritmo diseñado para conseguir combinaciones de modelos a partir de una familia inicial, provocando una disminución de la varianza y evitando el sobreajuste. Aunque lo más común es aplicarlo con los métodos basados en árboles de decisión, se puede usar con cualquier familia.\n","\n","<img src='https://drive.google.com/uc?export=view&id=1mjErfNYg71E27xhpOTMEowGWdt377dGY' width=\"600\" align=\"center\" style=\"margin-right: 20px\">\n","\n","La técnica consiste en lo siguiente:\n","\n","1. Dado un conjunto de entrenamiento, $D$, de tamaño $n$, el bagging genera $m$ nuevos conjuntos de entrenamiento, $D_{i}$, de tamaño $n'$, tomando al azar elementos de $D$ de manera uniforme y con reemplazo, por tanto, algunos elementos del conjunto original pueden aparecer repetidos en los nuevos conjuntos generados.\n","2. Si $n'=n$, entonces para valores de $n$ suficientemente grandes, se espera que cada $D_{i}$ tenga una fracción de $(1 - 1/e)$ elementos únicos de $D$, y el resto son duplicados.\n","3. A partir de estos $m$ nuevos conjuntos de entrenamiento se construyen $m$ nuevos modelos de aprendizaje, y la respuesta final de la combinación se consigue por medio de votación de las $m$ respuestas (en caso de buscar una clasificación) o por la media de ellas (en caso de buscar una regresión).\n","\n","Se ha probado que el bagging tiende a producir mejoras en los casos de modelos individuales inestables (como es el caso de las redes neuronales o los árboles de decisión), pero puede producir resultados mediocres o incluso empeorar los resultados con otros métodos, como el de los KNN."]},{"cell_type":"markdown","metadata":{"id":"IpHClY5aajRb"},"source":["## <font color='blue'>**Boosting**</font>\n","\n","A diferencia del bagging, en el **boosting** no se crean versiones del conjunto de entrenamiento, sino que se trabaja siempre con el conjunto completo de entrada, y se manipulan los pesos de los datos para generar modelos distintos.\n","\n","La idea es que en cada iteración se incremente el peso de los objetos mal clasificados por el predictor en esa iteración, por lo que en la construcción del próximo predictor estos objetos serán más importantes y será más probable clasificarlos bien.\n","\n","El método de boosting más famoso es el que se conoce como AdaBoost que consta de los siguientes pasos:\n","\n","1. Inicialmente, a todos los datos del conjunto de entrenamiento se les asigna un peso idéntico, $w_{i}=1/n$, donde $n$ es el tamaño del conjunto de datos.\n","2. Se entrena el modelo usando el conjunto de entrenamiento.\n","3. Se calcula el error del modelo en el conjunto de entrenamiento, se cuentan cuántos objetos han sido mal clasificados y se identifican cuáles son.\n","4. Se incrementan los pesos en aquellos casos de entrenamiento en los que el modelo anterior ha dado resultados erróneos.\n","5. Se entrena un nuevo modelo usando el conjunto de pesos modificados.\n","6. Volver al punto 3 (y se repite el proceso hasta el número de iteraciones fijadas inicialmente).\n","7. El modelo final se consigue por votación ponderada usando los pesos de todos los modelos.\n","\n","<img src='https://drive.google.com/uc?export=view&id=13N6yrydN76DdYd3cfuZMqJ-xC9A9LvgH' width=\"600\" align=\"center\" style=\"margin-right: 20px\">\n","\n","Concretamente, para modificar los pesos tras el cálculo del predictor Mt con los pesos en el tiempo t, wi,t se utilizan las siguientes ecuaciones:\n","\n","<img src='https://drive.google.com/uc?export=view&id=1s61wb4JW9ffmvxt6uvdBh5GjT7BwzhEw' width=\"300\" align=\"center\" style=\"margin-right: 20px\">\n","\n","donde $x_{i}$ es el vector de entrada del objeto, $y_{i}$ es la clase del objeto i-ésimo, y $M_{t}(x_{i})$ es la predicción del modelo para la entrada $x_{i}$.\n","\n","Tras esto, los pesos son actualizados de la siguiente forma:\n","\n","<img src='https://drive.google.com/uc?export=view&id=1d_ftfhK_cjmFELm9uBPBtZ6b1hwwWnOP' width=\"300\" align=\"center\" style=\"margin-right: 20px\">\n","\n","donde $c$ es una constante de normalización elegida de forma que\n","\n","<img src='https://drive.google.com/uc?export=view&id=16g9K6LXVFJJVFpNXNw4LhcyD86EEhM4a' width=\"150\" align=\"center\" style=\"margin-right: 20px\">\n","\n","La combinación construida clasifica por medio del voto de la mayoría, ponderando cada modelo $M_{t}$ por medio de $α_{t}$. Es decir:\n","\n","<img src='https://drive.google.com/uc?export=view&id=1ga35nM8MvLNBUdRqwu41eKopJc_pkWAA' width=\"300\" align=\"center\" style=\"margin-right: 20px\">\n","\n","Normalmente, se espera una mejora significativa sobre la clasificación producida por cada uno de los modelos individuales, pero la convergencia no está garantizada y el rendimiendo podría degradarse tras un cierto número de pasos.\n","\n","<img src='https://drive.google.com/uc?export=view&id=1vmM6FveB4_fvWz2I6PI6jMUDuBXwkIdh' width=\"600\" align=\"center\" style=\"margin-right: 20px\">"]},{"cell_type":"markdown","metadata":{"id":"_6qCzgqYeXWL"},"source":["## ¿Cuál es mejor, Bagging o Boosting?\n","\n","No hay un ganador absoluto, depende de los datos, la simulación y las circunstancias. Ambos disminuyen la varianza de su estimación única, ya que combinan varias estimaciones de diferentes modelos, así que el resultado puede ser un modelo con mayor estabilidad.\n","\n","Si el problema es que el modelo único obtiene un rendimiento muy bajo, Bagging rara vez obtendrá un mejor sesgo. Sin embargo, Boosting podría generar un modelo combinado con errores más bajos, ya que optimiza las ventajas y reduce las dificultades del modelo único.\n","\n","Por el contrario, si la dificultad del modelo único se adapta en exceso, entonces la mejor opción es Bagging. Boosting por su parte no ayuda a evitar el sobreajuste, de hecho, esta técnica se enfrenta a este problema en sí, por esta razón, Bagging es efectivo más a menudo que Boosting."]},{"cell_type":"markdown","metadata":{"id":"eDUaE2ArwTaL"},"source":["## Ventajas\n","\n","  * Son capaces de seleccionar predictores de forma automática.\n","  * Pueden aplicarse a problemas de regresión y clasificación.\n","  * Los árboles pueden, en teoría, manejar tanto predictores numéricos como categóricos sin tener que crear variables dummy o one-hot-encoding. En la práctica, esto depende de la implementación del algoritmo que tenga cada librería.\n","  * Al tratarse de métodos no paramétricos, no es necesario que se cumpla ningún tipo de distribución específica.\n","  * Por lo general, requieren mucha menos limpieza y pre procesado de los datos en comparación a otros métodos de aprendizaje estadístico (por ejemplo, no requieren estandarización).\n","  * No se ven muy influenciados por *outliers*.\n","  * Si para alguna observación, el valor de un predictor no está disponible, a pesar de no poder llegar a ningún nodo terminal, se puede conseguir una predicción empleando todas las observaciones que pertenecen al último nodo alcanzado. La precisión de la predicción se verá reducida pero al menos podrá obtenerse.\n","  * Son muy útiles en la exploración de datos, permiten identificar de forma rápida y eficiente las variables (predictores) más importantes.\n","  * Tienen buena escalabilidad, pueden aplicarse a conjuntos de datos con un elevado número de observaciones.\n","\n","## Desventajas\n","\n","  * Al combinar múltiples árboles, se pierde la interpretabilidad que tienen los modelos basados en un único árbol.\n","  * Cuando tratan con predictores continuos, pierden parte de su información al categorizarlas en el momento de la división de los nodos.\n","  * La creación de las ramificaciones de los árboles se consigue mediante el algoritmo de recursive binary splitting. Este algoritmo identifica y evalúa las posibles divisiones de cada predictor acorde a una determinada medida (RSS, Gini, entropía, entre otros). Los predictores continuos o predictores cualitativos con muchos niveles tienen mayor probabilidad de contener, solo por azar, algún punto de corte óptimo, por lo que suelen verse favorecidos en la creación de los árboles.\n","  * No son capaces de extrapolar fuera del rango de los predictores observado en los datos de entrenamiento.\n"]},{"cell_type":"markdown","metadata":{"id":"UrZWuYcd5_my"},"source":["## Gradient Boosting en Python\n","\n","Debido a sus buenos resultados, **Gradient Boosting** se ha convertido en el algoritmo de referencia cuando se trata con datos tabulares, de ahí que se hayan desarrollado múltiples implementaciones. Cada una tiene unas características que las hacen más adecuadas dependiendo del caso de uso. [Scikit-learn](https://scikit-learn.org/stable/index.html), tiene dos implementaciones nativas:\n","\n","```GradientBoostingClassifier``` y ```GradientBoostingRegressor```: son las primeras implementaciones que se hicieron de Gradient Boosting en scikit-learn.\n","\n","* No realiza binning\n","* Utiliza un único core (no paraleliza ninguna de las partes del algoritmo)\n","* Permite trabajar sobre matrices sparse\n","* Necesario hacer one-hot-encoding de variables categóricas\n","\n","```HistGradientBoostingClassifier``` y ```HistGradientBoostingRegressor```: nueva implementación inspirada en LightGBM. Esta segunda implementación tiene muchas más ventajas que la original, principalmente, su rapidez.\n","\n","* Sí realiza binning\n","* Multicore (paraleliza algunas partes del algoritmo)\n","* No permite trabajar sobre matrices sparse\n","* Permite que las observaciones incluyan valores missing\n","* Permite restricciones monotónicas\n","* No es necesario one-hot-encoding de variables categóricas (en construcción).\n","\n","Además de las opciones nativas, scikit-learn permite acceder a otras de las principales implementaciones de Gradient Boosting disponibles en Python:\n","\n","```LightGBM```:\n","\n","* Permite que las observaciones incluyan valores missing\n","* Permite el uso de GPUs\n","* Entrenamiento paralelizado (paraleliza algunas partes del algoritmo)\n","* Permite restricciones monotónicas\n","* Permite trabajar sobre matrices sparse\n","* No es necesario one-hot-encoding de variables categóricas\n","\n","```XGBoost```:\n","\n","* Permite que las observaciones incluyan valores missing\n","* Permite el uso de GPUs\n","* Entrenamiento paralelizado (paraleliza algunas partes del algoritmo)\n","* Permite restricciones monotónicas\n","* Permite trabajar sobre matrices sparse\n","* Necesario one-hot-encoding de variables categóricas\n","\n","```CatBoost```:\n","\n","* Optimizado principalmente para variables categóricas\n","* Utiliza árboles simétricos\n","* Permite que las observaciones incluyan valores missing\n","* Permite el uso de GPUs\n","* Entrenamiento paralelizado (paraleliza algunas partes del algoritmo)\n","* Permite restricciones monotónicas\n","\n","```H2O```:\n","\n","* Sí realiza binning\n","* Multicore (paraleliza algunas partes del algoritmo)\n","* Permite que las observaciones incluyan valores missing\n","* Permite restricciones monotónicas\n","* No es necesario one-hot-encoding de variables categóricas\n","\n","Si bien todas estas librerías disponen de un API para utilizarlas a través de scikit-learn, algunas de sus funcionalidades son accesibles de esta forma, por lo que, para un uso más preciso, es recomendable utilizar su API nativa.\n","\n","Otro aspecto a tener en cuenta es el modo en que tratan las variables categóricas (con o sin one-hot-encoding). Esto tiene impacto directo en la estructura de los árboles generados y, en consecuencia, en los resultados predictivos del modelo y en la importancia calculada para los predictores."]},{"cell_type":"markdown","metadata":{"id":"DeIPIyBEsNWu"},"source":["## <font color='blue'>**Gradient Boosted Regression Trees**</font>\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"RU8XYZecsNWv"},"source":["class Estimator(object):\n","\n","    def fit(self, X, y=None):\n","        \"\"\"Fits estimator to data. \"\"\"\n","        # set state of ``self``\n","        return self\n","\n","    def predict(self, X):\n","        \"\"\"Predict response of ``X``. \"\"\"\n","        # compute predictions ``pred``\n","        return pred"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lw-E1uLAsNWw"},"source":["Scikit-learn proporciona dos estimadores para Gradient Boosting: `` GradientBoostingClassifier`` y `` GradientBoostingRegressor``, ambos se encuentran en el paquete `` sklearn.ensemble``:"]},{"cell_type":"code","metadata":{"id":"fn9Grp7YsNWw"},"source":["from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import GradientBoostingRegressor"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iFEfNOOGsNWy"},"source":["Los estimadores tienen argumentos para controlar el comportamiento de ajuste; estos argumentos a menudo se denominan _hiperparámetros_. Entre los más importantes para GBRT se encuentran:\n","\n","  * número de árboles de regresión (o clasificación) (`` n_estimators`` )\n","  * profundidad de cada árbol individual (`` max_depth`` )\n","  * función de pérdida (``loss`` )\n","\n","Por ejemplo, si desea ajustar un modelo de regresión con 100 árboles de profundidad 3 utilizando mínimos cuadrados:"]},{"cell_type":"code","metadata":{"id":"pqkw5jrwsNWz"},"source":["est = GradientBoostingRegressor(n_estimators=100, max_depth=3, loss='ls')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n9O2gfeYsNWz"},"source":["est?"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZY2eYOagsNW0"},"source":["Aquí hay un ejemplo independiente que muestra cómo ajustar un `` GradientBoostingClassifier``  a un conjunto de datos sintéticos:"]},{"cell_type":"code","metadata":{"id":"GGCn1qWGsNW0"},"source":["from sklearn.datasets import make_hastie_10_2\n","from sklearn.model_selection import train_test_split\n","\n","# generate synthetic data from ESLII - Example 10.2\n","X, y = make_hastie_10_2(n_samples=5000)\n","X_train, X_test, y_train, y_test = train_test_split(X, y)\n","\n","# fit estimator\n","est2 = GradientBoostingClassifier(n_estimators=200, max_depth=3)\n","est2.fit(X_train, y_train)\n","\n","# predict class labels\n","pred = est2.predict(X_test)\n","\n","# score on test data (accuracy)\n","acc = est2.score(X_test, y_test)\n","print('ACC: %.4f' % acc)\n","\n","# predict class probabilities\n","est2.predict_proba(X_test)[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6vS2r-lxsNW1"},"source":["El estado del estimador se almacena en atributos de instancia que tienen un guión bajo al final ('\\ _'). Por ejemplo, la secuencia de árboles de regresión (objetos `` DecisionTreeRegressor `` ) se almacena en `` est.estimators_``:"]},{"cell_type":"code","metadata":{"id":"ayhCLy80sNW2"},"source":["est2.estimators_[0, 0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pbCiHTEbDAPl"},"source":["## <font color='green'>**Actividad 1**</font>"]},{"cell_type":"markdown","metadata":{"id":"50KbqA-bDfoi"},"source":["Entrene un Gradient Boosting Classifier para los siguientes datos:"]},{"cell_type":"code","metadata":{"id":"83MPhc8_DmKb"},"source":["from sklearn.datasets import load_digits\n","digits = load_digits()\n","X=digits.data\n","y=digits.target"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GJZAOtuWD2d8"},"source":["Haga una separacion train test de 70-30 y entrene un modelo con 200 estimadores."]},{"cell_type":"code","metadata":{"id":"6jnJVaWlDzyg"},"source":["# Tú codigo aquí"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jo9xe-waDGcd"},"source":["<font color='green'>Fin Actividad 1</font>"]},{"cell_type":"markdown","metadata":{"id":"LITEkWvqsNW2"},"source":["## <font color='blue'>**Aplicando Gradient Boosted Regression Trees**</font>\n","\n","Aproximación de una función\n","\n","  * Función sinusoidal + ruido gaussiano\n","  * Entrenamiento: puntos azules\n","  * Test: puntos rojos"]},{"cell_type":"code","metadata":{"id":"AOgkydowsNW3"},"source":["%pylab inline\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","FIGSIZE = (11, 7)\n","\n","def ground_truth(x):\n","    \"\"\"Ground truth -- function to approximate\"\"\"\n","    return x * np.sin(x) + np.sin(2 * x)\n","\n","def gen_data(n_samples=200):\n","    \"\"\"generate training and testing data\"\"\"\n","    np.random.seed(15)\n","    X = np.random.uniform(0, 10, size=n_samples)[:, np.newaxis]\n","    y = ground_truth(X.ravel()) + np.random.normal(scale=2, size=n_samples)\n","    # train_mask = np.random.randint(0, 2, size=n_samples).astype(np.bool)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n","    return X_train, X_test, y_train, y_test\n","\n","X_train, X_test, y_train, y_test = gen_data(100)\n","\n","# plot ground truth\n","x_plot = np.linspace(0, 10, 500)\n","\n","def plot_data(alpha=0.4, s=20):\n","    fig = plt.figure(figsize=FIGSIZE)\n","    gt = plt.plot(x_plot, ground_truth(x_plot), alpha=alpha, label='ground truth')\n","\n","    # plot training and testing data\n","    plt.scatter(X_train, y_train, s=s, alpha=alpha)\n","    plt.scatter(X_test, y_test, s=s, alpha=alpha, color='red')\n","    plt.xlim((0, 10))\n","    plt.ylabel('y')\n","    plt.xlabel('x')\n","\n","\n","annotation_kw = {'xycoords': 'data', 'textcoords': 'data',\n","                 'arrowprops': {'arrowstyle': '->', 'connectionstyle': 'arc'}}\n","\n","plot_data()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SOgmokDRsNW4"},"source":["## Regression Trees\n","\n","  * El argumento `` max_depth `` controla la profundidad del árbol\n","  * Cuanto más profundo es el árbol, más varianza se puede explicar"]},{"cell_type":"code","metadata":{"id":"NgxqZBhisNW4"},"source":["from sklearn.tree import DecisionTreeRegressor\n","plot_data()\n","est = DecisionTreeRegressor(max_depth=1).fit(X_train, y_train)\n","plt.plot(x_plot, est.predict(x_plot[:, np.newaxis]),\n","         label='RT max_depth=1', color='r', alpha=0.9, linewidth=2)\n","\n","est = DecisionTreeRegressor(max_depth=3).fit(X_train, y_train)\n","plt.plot(x_plot, est.predict(x_plot[:, np.newaxis]),\n","         label='RT max_depth=3', color='g', alpha=0.7, linewidth=1)\n","\n","est = DecisionTreeRegressor(max_depth=7).fit(X_train, y_train)\n","plt.plot(x_plot, est.predict(x_plot[:, np.newaxis]),\n","         label='RT max_depth=7', color='b', alpha=0.7, linewidth=1)\n","\n","plt.legend(loc='upper left')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Djot11l1sNW4"},"source":["## Aproximación de la función con Gradient Boosting\n","\n","  * El argumento `` n_estimators`` controla el número de árboles\n","  * El método `` staged_predict`` nos permite recorrer las predicciones a medida que agregamos más árboles"]},{"cell_type":"code","metadata":{"id":"V1h7ejCJsNW4"},"source":["from itertools import islice\n","\n","plot_data()\n","# !00 estimadores con árboles de poca profundidad\n","est = GradientBoostingRegressor(n_estimators=1000, max_depth=1, learning_rate=1.0)\n","est.fit(X_train, y_train)\n","\n","ax = plt.gca()\n","first = True\n","\n","# step through prediction as we add 10 more trees.\n","# Boosting nos da un método llamado staged_predict, es una lista que predice para cada uno de\n","# los estimadores. Esto se puede hacer poruqe van en serie, en línea\n","# parte en 0 hasta n_estimatos, y avanza de 10 en 10\n","for pred in islice(est.staged_predict(x_plot[:, np.newaxis]), 0, est.n_estimators, 10):\n","    # Se plote cada 10 en amarillo\n","    plt.plot(x_plot, pred, color='y', alpha=0.2)\n","    if first:\n","        # Annotate para el primer árbol, obviamente será underfitting\n","        ax.annotate('High bias - low variance', xy=(x_plot[x_plot.shape[0] // 2],\n","                                                    pred[x_plot.shape[0] // 2]),\n","                                                    xytext=(4, 4), **annotation_kw)\n","        first = False\n","\n","pred = est.predict(x_plot[:, np.newaxis])\n","# Esta es la estimación final\n","plt.plot(x_plot, pred, color='r', label='GBRT max_depth=1')\n","ax.annotate('Low bias - high variance', xy=(x_plot[x_plot.shape[0] // 2],\n","                                            pred[x_plot.shape[0] // 2]),\n","                                            xytext=(6.25, -6), **annotation_kw)\n","plt.legend(loc='upper left')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9R-C3D3LsNW4"},"source":["## Complejidad del modelo\n","\n","  * El número de árboles y la profundidad de los árboles individuales controlan la complejidad del modelo\n","  * La complejidad del modelo tiene un precio: **sobreajuste**\n","  \n","  \n","## Gráfico de desviación\n","\n","  * Diagnóstico para determinar si el modelo está sobreajustado\n","  * Traza el error de entrenamiento / prueba (desviación) en función del número de árboles (= complejidad del modelo)\n","  * El error de entrenamiento (desviación) se almacena en `` est.train_score_``\n","  * El error de prueba se calcula usando ``est.staged_predict ``"]},{"cell_type":"code","metadata":{"id":"N82wOUhXsNW5"},"source":["def deviance_plot(est, X_test, y_test, ax=None, label='', train_color='#2c7bb6',\n","                  test_color='#d7191c', alpha=1.0, ylim=(0, 10)):\n","    \"\"\"Deviance plot for ``est``, use ``X_test`` and ``y_test`` for test error. \"\"\"\n","    n_estimators = len(est.estimators_)\n","    test_dev = np.empty(n_estimators)\n","\n","    for i, pred in enumerate(est.staged_predict(X_test)):\n","      # Se calcula el error\n","       test_dev[i] = est.loss_(y_test, pred)\n","\n","    if ax is None:\n","        fig = plt.figure(figsize=FIGSIZE)\n","        ax = plt.gca()\n","\n","    ax.plot(np.arange(n_estimators) + 1, test_dev, color=test_color, label='Test %s' % label,\n","             linewidth=2, alpha=alpha)\n","    ax.plot(np.arange(n_estimators) + 1, est.train_score_, color=train_color,\n","             label='Train %s' % label, linewidth=2, alpha=alpha)\n","    ax.set_ylabel('Error')\n","    ax.set_xlabel('n_estimators')\n","    ax.set_ylim(ylim)\n","    return test_dev, ax\n","\n","test_dev, ax = deviance_plot(est, X_test, y_test)\n","ax.legend(loc='upper right')\n","\n","# add some annotations\n","ax.annotate('Lowest test error', xy=(test_dev.argmin() + 1, test_dev.min() + 0.02),\n","            xytext=(150, 3.5), **annotation_kw)\n","\n","ann = ax.annotate('', xy=(800, test_dev[799]),  xycoords='data',\n","                  xytext=(800, est.train_score_[799]), textcoords='data',\n","                  arrowprops={'arrowstyle': '<->'})\n","ax.text(810, 3.5, 'train-test gap')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["La línea azul es la que se calcula sobre el set de training. El modelo aprende hasta que se sobre ajusta.\n","La roja representa el set de test. En algún punto, si el método no tiene elementos fiuertes de regularización, deja de aprender y empieza a tener más error.\n","\n","Esta curva se puede hacer porque el método de boosting tiene iteraciones. Con los Random Forest no se puede hacer porque se ejecuta en paralelo.\n","\n","En el caso de las redes neuronales se puede hacer este tipo de gráficos basado en los epochs.\n","\n","Todas las técnicas con iteraciones pueden construir este tipo de gráficos."],"metadata":{"id":"ljQOkUDaaPzv"}},{"cell_type":"markdown","metadata":{"id":"3Zxge6w7sNW6"},"source":["## Sobreajuste (Overfitting)\n","\n","  * El modelo tiene demasiada capacidad y comienza a ajustarse a los datos de entrenamiento\n","  * Indicado por una gran brecha entre el error de entrenamiento y el error de prueba\n","  * GBRT proporciona una serie de parametros para controlar el sobreajuste"]},{"cell_type":"markdown","metadata":{"id":"Flifna2VsNW6"},"source":["## Regularización\n","\n","  * Estructura de árbol\n","  * Contracción\n","  * Impulso de gradiente estocástico\n","\n","## Estructura de árbol\n","\n","  * El `` max_depth``  de los árboles controla el grado de interacciones de características (varianza ++)\n","  * Utilice `` min_samples_leaf``  para tener una cantidad suficiente de muestras por hoja (sesgo ++)"]},{"cell_type":"code","metadata":{"id":"aJ41jsjLsNW6"},"source":["def fmt_params(params):\n","    return \", \".join(\"{0}={1}\".format(key, val) for key, val in params.items())\n","\n","fig = plt.figure(figsize=FIGSIZE)\n","ax = plt.gca()\n","for params, (test_color, train_color) in [({}, ('#d7191c', '#2c7bb6')),\n","                                          ({'min_samples_leaf': 3}, ('#fdae61', '#abd9e9'))]:\n","    est = GradientBoostingRegressor(n_estimators=1000, max_depth=1,\n","                                    learning_rate=1.0)\n","    est.set_params(**params)\n","    est.fit(X_train, y_train)\n","    test_dev, ax = deviance_plot(est, X_test, y_test, ax=ax, label=fmt_params(params),\n","                                 train_color=train_color, test_color=test_color)\n","\n","ax.annotate('Higher bias', xy=(900, est.train_score_[899]), xytext=(600, 3), **annotation_kw)\n","ax.annotate('Lower variance', xy=(900, test_dev[899]), xytext=(600, 3.5), **annotation_kw)\n","plt.legend(loc='upper right')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c6JLe5xKsNW6"},"source":["## Contracción\n","\n"," * Aprendizaje lento al reducir las predicciones de cada árbol en un pequeño escalar (`` learning_rate`` )\n"," * Una ``learning_rate``  más baja requiere una mayor cantidad de ``n_estimadores``\n"," * Es una compensación entre tiempo de ejecución y precisión."]},{"cell_type":"code","metadata":{"id":"5OjvrYVasNW7"},"source":["fig = plt.figure(figsize=FIGSIZE)\n","ax = plt.gca()\n","for params, (test_color, train_color) in [({}, ('#d7191c', '#2c7bb6')),\n","                                          ({'learning_rate': 0.1},\n","                                           ('#fdae61', '#abd9e9'))]:\n","    est = GradientBoostingRegressor(n_estimators=1000, max_depth=1, learning_rate=1.0)\n","    est.set_params(**params)\n","    est.fit(X_train, y_train)\n","\n","    test_dev, ax = deviance_plot(est, X_test, y_test, ax=ax, label=fmt_params(params),\n","                                 train_color=train_color, test_color=test_color)\n","\n","ax.annotate('Requires more trees', xy=(200, est.train_score_[199]),\n","            xytext=(300, 1.75), **annotation_kw)\n","ax.annotate('Lower test error', xy=(900, test_dev[899]),\n","            xytext=(600, 1.75), **annotation_kw)\n","plt.legend(loc='upper right')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0x-Hz3xfsNW7"},"source":["## Stochastic Gradient Boosting\n","\n"," * Submuestreo del conjunto de entrenamiento antes de hacer crecer cada árbol (`` subsample ``)\n"," * Submuestreo de las características antes de encontrar el mejor nodo dividido (`` max_features ``)\n"," * La última suele funcionar mejor si hay una cantidad suficiente de funciones"]},{"cell_type":"code","metadata":{"id":"0d230qiasNW7"},"source":["fig = plt.figure(figsize=FIGSIZE)\n","ax = plt.gca()\n","# Incorporamos el 'subsample'\n","for params, (test_color, train_color) in [({}, ('#d7191c', '#2c7bb6')),\n","                                          ({'learning_rate': 0.1, 'subsample': 0.5},\n","                                           ('#fdae61', '#abd9e9'))]:\n","    est = GradientBoostingRegressor(n_estimators=1000, max_depth=1, learning_rate=1.0,\n","                                    random_state=1)\n","    est.set_params(**params)\n","    est.fit(X_train, y_train)\n","    test_dev, ax = deviance_plot(est, X_test, y_test, ax=ax, label=fmt_params(params),\n","                                 train_color=train_color, test_color=test_color)\n","\n","ax.annotate('Even lower test error', xy=(400, test_dev[399]),\n","            xytext=(500, 3.0), **annotation_kw)\n","\n","est = GradientBoostingRegressor(n_estimators=1000, max_depth=1, learning_rate=1.0,\n","                                subsample=0.5)\n","est.fit(X_train, y_train)\n","test_dev, ax = deviance_plot(est, X_test, y_test, ax=ax, label=fmt_params({'subsample': 0.5}),\n","                             train_color='#abd9e9', test_color='#fdae61', alpha=0.5)\n","ax.annotate('Subsample alone does poorly', xy=(300, test_dev[299]),\n","            xytext=(500, 5.5), **annotation_kw)\n","plt.legend(loc='upper right', fontsize='small')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a4ejiTlksNW8"},"source":["## <font color='blue'>**Ajuste de Hiperparámetros**</font>\n","\n","Para ajustar los hiperparámetros se puede realizar el siguiente proceso:\n","\n","  1. Elija `` n_estimators``  tan grande como sea posible (computacionalmente) (por ejemplo, 3000)\n","  2. Sintonice `` max_depth``, `` learning_rate``, `` min_samples_leaf`` y `` max_features`` mediante GridSearch\n","  3. Aumente `` n_estimators`` aún más y ajuste ``learning_rate`` nuevamente manteniendo los otros parámetros fijos"]},{"cell_type":"code","metadata":{"id":"lCoTR-HYsNW8"},"source":["from sklearn.model_selection import GridSearchCV\n","\n","param_grid = {'learning_rate': [0.1, 0.01, 0.001],\n","              'max_depth': [4, 6],\n","              'min_samples_leaf': [3, 5],  ## depends on the nr of training examples\n","               'max_features': [1.0, 0.3, 0.1] ## not possible in our example (only 1 fx)\n","              }\n","\n","est = GradientBoostingRegressor(n_estimators=3000)\n","# this may take some minutes\n","gs_cv = GridSearchCV(est, param_grid, scoring='neg_mean_squared_error', n_jobs=4).fit(X_train, y_train)\n","\n","# best hyperparameter setting\n","print('Best hyperparameters: %r' % gs_cv.best_params_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZACozJhFsNW8"},"source":["# refit model on best parameters\n","est.set_params(**gs_cv.best_params_)\n","est.fit(X_train, y_train)\n","\n","# plot the approximation\n","plot_data()\n","plt.plot(x_plot, est.predict(x_plot[:, np.newaxis]), color='r', linewidth=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZwijvP9SsNW9"},"source":["## <font color='blue'>**Ejemplo: California Housing**</font>\n","\n","* Predecir el valor medio de la vivienda para los grupos de bloques del censo en California\n"," * Base de datos: 20.000 grupos, 8 características: *ingreso medio*, *edad promedio de la vivienda*, *latitud*, *longitud*, ...\n"," * Métrica:Error absoluto medio en el conjunto de prueba (20% de la BD)"]},{"cell_type":"code","metadata":{"id":"XTKk4OjMsNW9"},"source":["from sklearn.datasets import fetch_california_housing\n","\n","cal_housing = fetch_california_housing()\n","\n","# split 80/20 train-test\n","X_train, X_test, y_train, y_test = train_test_split(cal_housing.data,\n","                                                    cal_housing.target,\n","                                                    test_size=0.2,\n","                                                    random_state=1)\n","names = cal_housing.feature_names"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v4PtcIxFC_8K"},"source":["names"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eZeoLScLsNW-"},"source":["Desafíos:\n","\n","  * características heterogéneas (diferentes escalas y distribuciones, ver gráfico a continuación)\n","  * interacciones de características no lineales (interacción: latitud y longitud)\n","  * respuestas extremas (técnicas de regresión robustas)\n"]},{"cell_type":"code","metadata":{"id":"8TJd2gkgsNW-"},"source":["import pandas as pd\n","X_df = pd.DataFrame(data=X_train, columns=names)\n","X_df['MedHouseVal'] = y_train\n","_ = X_df.hist(column=['Latitude', 'Longitude', 'MedInc', 'MedHouseVal'], figsize=FIGSIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ud883qS4sNW-"},"source":["## Evaluación\n","\n","  * GBRT vs RandomForest vs SVM vs Ridge Regression"]},{"cell_type":"code","metadata":{"id":"YVA4KfFesNW_"},"source":["import time\n","from collections import defaultdict\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.linear_model import Ridge\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.dummy import DummyRegressor\n","from sklearn.svm import SVR\n","\n","res = defaultdict(dict)\n","\n","def benchmark(est, name=None):\n","    if not name:\n","        name = est.__class__.__name__\n","    t0 = time.clock()\n","    est.fit(X_train, y_train)\n","    res[name]['train_time'] = time.clock() - t0\n","    t0 = time.clock()\n","    pred = est.predict(X_test)\n","    res[name]['test_time'] = time.clock() - t0\n","    res[name]['MAE'] = mean_absolute_error(y_test, pred)\n","    return est\n","\n","benchmark(DummyRegressor())\n","benchmark(Ridge(alpha=0.0001, normalize=True))\n","benchmark(Pipeline([('std', StandardScaler()),\n","                    ('svr', SVR(kernel='rbf', C=10.0, gamma=0.1, tol=0.001))]), name='SVR')\n","benchmark(RandomForestRegressor(n_estimators=100, max_features=5, random_state=0,\n","                                bootstrap=False, n_jobs=4))\n","est = benchmark(GradientBoostingRegressor(n_estimators=500, max_depth=4, learning_rate=0.1,\n","                                          loss='huber', min_samples_leaf=3,\n","                                          random_state=0))\n","\n","res_df = pd.DataFrame(data=res).T\n","res_df[['train_time', 'test_time', 'MAE']].sort_values('MAE', ascending=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gRBiKRpksNW_"},"source":["## <font color='green'>Actividad 2</font>\n","\n","El `` GradientBoostingRegressor``  anterior no está ajustado correctamente para este conjunto de datos. Diagnostique el modelo actual y encuentre configuraciones de hiperparámetros más apropiadas."]},{"cell_type":"code","source":["# Tú codigo aquí"],"metadata":{"id":"Qxde5dSPPotS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ATT2kB_gERpw"},"source":["<font color='green'>Fin Actividad 2</font>"]},{"cell_type":"markdown","metadata":{"id":"tjKH1_OysNW_"},"source":["## Importancia de las características\n","\n","  * ¿Cuáles son las características importantes y cómo contribuyen a predecir la respuesta objetivo?\n","  * Derivado de los árboles de regresión\n","  * Se puede acceder a través del atributo `` est.feature_importances_``"]},{"cell_type":"code","metadata":{"id":"uyxAjqhHsNXA"},"source":["fx_imp = pd.Series(est.feature_importances_, index=names)\n","fx_imp /= fx_imp.max()  # normalize\n","fx_imp.sort_values()\n","fx_imp.plot(kind='barh', figsize=FIGSIZE)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e6iAQu-nsNXA"},"source":["## Dependencia parcial\n","\n","  * Relación entre la respuesta y un conjunto de características, marginando todas las demás características\n","  * Intuitivamente: respuesta esperada en función de las características a las que condicionamos"]},{"cell_type":"code","metadata":{"id":"_JDonRRJsNXA"},"source":["from sklearn.inspection import plot_partial_dependence\n","\n","features = ['MedInc', 'AveOccup', 'HouseAge',\n","            ('AveOccup', 'HouseAge')]\n","plot_partial_dependence(est, X_train, features, feature_names=names, n_cols=2) #, figsize=FIGSIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.inspection import plot_partial_dependence\n","\n","features = ['MedInc', 'AveOccup', 'HouseAge',\n","            ('AveOccup', 'HouseAge')]\n","\n","plot_partial_dependence(est, X_train, features, feature_names=names, n_cols=2)\n","plt.show()"],"metadata":{"id":"nCWfCIj0oqhR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cDcJt2YisNXB"},"source":["## Resumen\n","\n"," - Técnica flexible de clasificación y regresión no paramétrica\n"," - Aplicable a una variedad de problemas\n"," - Implementación sólida en scikit-learn\n",""]},{"cell_type":"markdown","metadata":{"id":"q1TuGBTIsNXB"},"source":["## <font color='green'>Actividad 3</font>"]},{"cell_type":"markdown","metadata":{"id":"-bDPHsuAsNXD"},"source":["Para el siguiente dataset encuentre los mejores parametros con GridSearchCV e implemente la experimentacion con 5-fold cross-validation. Reporte el F1, precision y recall."]},{"cell_type":"code","metadata":{"id":"8rAJDp8msNXC"},"source":["from sklearn.datasets import fetch_covtype\n","X, y = fetch_covtype(return_X_y=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qDnAvCilsNXC"},"source":["# Tú codigo aquí"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3OLxXDB0EXh3"},"source":["<font color='green'>Fin Actividad 3</font>"]}]}