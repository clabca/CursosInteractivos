{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"dpnByx3tvUP3"},"source":["# **Aprendizaje supervisado**\n","# SL04. Bias & Variance"]},{"cell_type":"markdown","source":["## <font color='blue'>**Introducción Bias y Variance**</font>\n","\n","Cuando desarrollamos un modelo de Machine Learning, dedicamos gran parte de nuestro esfuerzo en lograr la mayor precisión posible, pero la realidad es que no se puede construir modelos 100% precisos ya que nunca pueden estar libres de errores.\n","\n","Comprender cómo las diferentes fuentes de error generan __bias__ (sesgo) y __variance__ (varianza) nos ayudara a mejorar el proceso de ajuste de datos, lo que resulta en modelos más precisos. Adicionalmente, también mos ayudará a evitar el error de sobreajuste (__overfitting__) y falta de ajuste (__underfitting__).\n","\n","En un sistema real, no podemos manejar simultáneamente el bias y el variance. Si se desea reducir el bias del modelo, aumentará el variance hasta cierto punto y viceversa. La razón fundamental de este fenómeno es que siempre queremos intentar estimar datos reales ilimitados con muestras de entrenamiento limitadas.\n","\n","Cuando creemos más en la autenticidad de estos datos e ignoramos el conocimiento previo del modelo, haremos todo lo posible para garantizar la precisión del modelo en las muestras de entrenamiento, lo que puede reducir el bias del modelo. Sin embargo, el modelo aprendido de esta manera puede perder cierta capacidad de generalización, lo que provocará un ajuste excesivo, reducirá el rendimiento del modelo en datos reales y aumentará la incertidumbre del modelo. Por el contrario, si creemos más en nuestro conocimiento previo del modelo y agregamos más restricciones al modelo en el proceso de aprendizaje del modelo, podemos reducir el variance del modelo y mejorar la estabilidad del modelo, pero también aumentar el bias del modelo. La compensación entre **bias** y **variance** es uno de los temas básicos del Machine Learning."],"metadata":{"id":"u5kpWcq2strw"}},{"cell_type":"markdown","source":["### Bias\n","El error debido al Bias de un modelo corresponde a la diferencia entre el valor esperado del estimador (es decir, la predicción media del modelo) y el valor real. Cuando se dice que un modelo tiene un bias muy alto quiere decir que el modelo es muy simple y no se ha ajustado a los datos de entrenamiento (underfitting), por lo que produce un error alto en todas los conjuntos (entrenamiento, validación y test).\n","\n","Los algoritmos de Machine Learning con __bajo bias__ incluyen: Decision Trees (árboles de decisión), k-Nearest Neighbors (KNN, k-vecinos más cercanos) y Support Vector Machines (SVM, máquinas de soporte vectorial). Por su parte, los algoritmos con __alto bia__s incluyen: Linear Regression (regresión lineal), Linear Discriminant Analysis (LDA, análisis discriminante lineal) y Logistic Regression (regresión logística).\n","\n","<img src='https://drive.google.com/uc?export=view&id=1Lexuleer-JexTzcq-JhJrIVBJJuh8Dgr' width=\"400\" align=\"center\" style=\"margin-right: 20px\">"],"metadata":{"id":"Mpjm6B0vDH9i"}},{"cell_type":"markdown","source":["### Variance\n","El _variance_ de un estimador es cuánto varía la predicción según los datos que utilicemos para el entrenamiento.\n","\n","La mayoría de algoritmos de Machine Learning aprenden según van entrando datos de entrenamiento. Así que es normal que todos los modelos tengan cierta varianza. Idealmente no debería cambiar demasiado de un conjunto de datos de entrenamiento a otro, lo que significa que el algoritmo es bueno para elegir el mapeo subyacente oculto entre las variables de entrada y de salida.\n","\n","Un modelo con bajo _variance_ indica que cambiar los datos de entrenamiento produce cambios pequeños en la estimación.\n","Al contrario, un modelo con alto _variance_ quiere decir que pequeños cambios en el dataset se traducen en grandes cambios en la salida (overfitting).\n","\n","Los algoritmos de Machine Learning que tienen un alto _variance_ están fuertemente influenciados por las particularidades de los datos de entrenamiento. Esto significa que los detalles del entrenamiento influyen en el número y tipo de parámetros utilizados para caracterizar la función de mapeo.\n","\n","En general, los algoritmos que tienen mucha flexibilidad (representan muy ien la data de entrenamiento) tienen un altp _variance_. Por ejemplo, los árboles de decisión tienen una variación alta, que es aún mayor si los árboles no se podan antes de usarlos.\n","\n","Los ejemplos de algoritmos de aprendizaje automático de __bajo variance__ incluyen: regresión lineal, análisis discriminante lineal y regresión logística.\n","\n","Los ejemplos de algoritmos de aprendizaje automático de __alto variance__ incluyen: árboles de decisión, k-vecinos más cercanos y máquinas de vectores de soporte.\n","\n","Los algoritmos que suelen tener un error de bias alto suelen tener una varianza baja. A su vez, los que tienen bias bajo suelen tener varianza alta.\n","\n","<img src='https://drive.google.com/uc?export=view&id=1i22nLMK2I1MjviCpurJnWZEjYnm0WWot' width=\"400\" align=\"center\" style=\"margin-right: 20px\">"],"metadata":{"id":"4FsYoL2wDDfh"}},{"cell_type":"markdown","source":["Al construir un modelo debemos apuntar a que **tenga poco bias y poco variance**. Esto es lo que se llamaría un **modelo robusto**.\n","\n","<img src='https://drive.google.com/uc?export=view&id=1rEwd7QgcoPyLcjtNJ2tMBbn_y8EKHrSJ' width=\"600\" align=\"center\" style=\"margin-right: 20px\">"],"metadata":{"id":"YqzeyH1iDRfc"}},{"cell_type":"markdown","metadata":{"id":"difpaWGWOw7k"},"source":["## <font color='blue'>**Definiendo la función y su error**</font>\n","\n","Consideremos el siguiente caso:\n","\n","1. $\\hat{x}$, es un conjunto de variables independientes\n","2. $y$, corresponde a la variable observada que puede tener un ruido $\\epsilon$ (error irreducible).\n","<br>\n","$$y = f(x) + \\epsilon$$\n","<br>\n","3. Las características de este ruido son:\n","<br>\n","$$ \\mathbb{E}[\\epsilon] = 0 \\\\\n"," \\mbox{var}(\\epsilon)= \\sigma_{\\epsilon}^2\n","$$\n","<br>\n","4. Si $\\hat{f}$ es la función estimada a través de una muestra de los datos\n","5. El error cuadrático medio (__MSE__) es:\n","$$\n","\\mbox{MSE} = \\mathbb{E}[(y-\\hat{f}(x))^2]\n","$$\n","6. El _bias_:\n","\n","$$\n","\\mbox{Bias}[\\hat{f}(x)] = \\mathbb{E}[\\hat{f}(x)] - f(x)\n","$$\n","7. El _variance_:\n","$$\n","\\mbox{var}(\\hat{f}(x))= \\mathbb{E}[(\\hat{f}(x)-\\mathbb{E}[\\hat{f}(x)])^2]\n","$$\n","\n","Con la información anterior, podemos definir el **Error Total** como:\n","<br><br>\n","$$\n","\\large\\color{green}{\\mathbb{E}[\\mathbb{E}[(y - \\hat{f}(x))^2]]} = \\color{magenta}{\\mathbb{E}[bias[\\hat{f}(x)]^2]} + \\color{blue}{\\mathbb{E}[var(\\hat{f}(x))]} + \\color{red}{\\sigma_e^2}\n","$$\n","<br>\n","<img src='https://drive.google.com/uc?export=view&id=1t0pgfGrF5IWKuaX9SeSrFmXa6x9kj77m' width=\"400\" align=\"center\" style=\"margin-right: 20px\">\n","\n","Un buen modelo predictivo será el que tenga un buen balance entre _bias_ y _variance_ de manera que se minimice el _error total_.\n"]},{"cell_type":"markdown","metadata":{"id":"HdBzjzwWEgQ1"},"source":["## <font color='blue'>**Contruccion del dataset**</font>\n","\n","\n","\n","```\n","def f(x):\n","    return .5 * x + np.sqrt(np.max(x, 0)) - np.cos(x) + 2\n","```\n","\n","\n","\n","```\n","def Ruido(x_max,x_test):\n","    \n","    x = x_max * (2 * np.random.rand(N) - 1)\n","    epsilon = sigma_epsilon * np.random.randn(N) # Media 0 and varianza 1\n","    y = f(x) + epsilon\n","    y_test = f(x_test) + sigma_epsilon * np.random.randn()\n","    return x, y , x_test, y_test\n","```\n","\n","\n","\n","```\n","def grafica(x,y):\n","    plt.figure(figsize=(12, 6))\n","    x_range = np.linspace(-x_max, x_max, 1000)\n","    plt.scatter(x, y) # Los puntos con ruido\n","    plt.plot(x_range, f(x_range), 'r', linewidth=3.0) # La función original\n","    plt.scatter(x_test, y_test, c='r') # El punto test\n","    plt.xlabel('x', size=12)\n","    plt.ylabel('y', size=12)\n","    plt.xticks(np.arange(-x_max, x_max + 1))\n","    plt.show()\n","```\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"pDB-DBy1Ow7l"},"source":["import numpy as np\n","import scipy.stats as stats\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","def f(x):\n","    return .5 * x + np.sqrt(np.max(x, 0)) - np.cos(x) + 2\n","\n","def Ruido(x_max,x_test):\n","\n","    x = x_max * (2 * np.random.rand(N) - 1)\n","    epsilon = sigma_epsilon * np.random.randn(N) # Media 0 and varianza 1\n","    y = f(x) + epsilon\n","    y_test = f(x_test) + sigma_epsilon * np.random.randn()\n","    return x, y , x_test, y_test\n","\n","def grafica(x,y):\n","    plt.figure(figsize=(12, 6))\n","    x_range = np.linspace(-x_max, x_max, 1000)\n","    plt.scatter(x, y, alpha=0.5) # Los puntos con ruido\n","    plt.plot(x_range, f(x_range), 'r', linewidth=3.0) # La función original\n","    plt.scatter(x_test, y_test, c='r', alpha=0.5) # El punto test\n","    plt.xlabel('x', size=12)\n","    plt.ylabel('y', size=12)\n","    plt.xticks(np.arange(-x_max, x_max + 1))\n","    plt.show()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P4P9q_zvE1F6"},"source":["sigma_epsilon = 1\n","N = 1000\n","x_max = 3\n","x_test = 3.2\n","x, y , x_test, y_test = Ruido(x_max,x_test)\n","grafica(x,y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d2u5sNgSOw7o"},"source":["### Modelemos el problema como una regresión\n","\n","$$ \\hat{f}(x) = \\omega_0 + \\omega_1x + \\omega_2 x^2 + ...+ \\omega_n x^n$$\n","\n","Es decir intentamos aproximar $y(x)$ con $\\hat{f}(x)$ como se describe en la ecuación anterior.\n","\n","Ahora, supongamos que solo podríamos usar 20 puntos (de los 1,000) para entrenar nuestro modelo de regresión polinómica y consideramos cuatro modelos de regresión diferentes, uno con grado $n = 1$ (línea simple), uno con $n = 2$, $n = 3$ y $n = 4$. Si tomamos muestras de forma aleatoria de 20 puntos de la población subyacente y repetimos este experimento 6 veces, este es un posible resultado que obtenemos."]},{"cell_type":"code","metadata":{"id":"0KLzO0lcOw7p"},"source":["def f_hat(x, w):\n","    d = len(w) - 1\n","    # Calcula el valor para los distintos polinomios.\n","    return np.sum(w * np.power(x, np.expand_dims(np.arange(d, -1, -1), 1)).T, 1)\n","\n","n = int(.02 * N) # n es el tamaño de la muestra, 20 para este ejemplo.\n","x_test = 3.2 # Puntoa verificar.\n","x_range = np.linspace(-x_max, x_max, 1000)\n","colors = np.array(['tab:green', 'tab:purple', 'tab:cyan', 'tab:orange'])\n","d_arr = [1, 2, 3, 4] # DImsensión del polinomio.\n","\n","cnt = 1\n","fig, axs = plt.subplots(2, 3, sharey=True, figsize=(15, 9))\n","# 6 casos\n","for i in range(2):\n","    for j in range(3):\n","        idx = np.random.permutation(N)[:n] # Obtenemos los indices de los 20 puntos en forma aleatoria\n","        x_train, y_train = x[idx], y[idx] # Obtenemos los 20 puntos.\n","\n","        w = []\n","        for d in d_arr: # Los distintos modelos.\n","            w.append(np.polyfit(x_train, y_train, d)) # Ajustamos a distintos grados de polinomios .\n","\n","        axs[i, j].scatter(x_train, y_train)\n","        axs[i, j].plot(x_range, f(x_range), 'r', linewidth=3.0)\n","        for k in range(len(w)):\n","            axs[i, j].plot(x_range, f_hat(x_range, w[k]), colors[k], linewidth=3.0)\n","\n","        axs[i, j].scatter(x_test, y_test, c='r')\n","        for k in range(len(w)):\n","            axs[i, j].scatter(x_test, f_hat(x_test, w[k]), c=colors[k])\n","\n","        axs[i, j].set_xlabel('x', size=12)\n","        axs[i, j].set_ylabel('y', size=12)\n","        axs[i, j].legend([r'$f$', r'$\\hat{f}$ (d = 1)', r'$\\hat{f}$ (d = 2)',\n","                          r'$\\hat{f}$ (d = 3)', r'$\\hat{f}$ (d = 4)'], fontsize=12)\n","        axs[i, j].title.set_text('experimento {}'.format(cnt))\n","        cnt += 1\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ENQtp2hEOw7r"},"source":["### Simulemos muchos casos\n","\n","Ahora, supongamos que simulamos 10,000 experimentos diferentes al muestrear aleatoriamente cada 20 puntos de la población subyacente para que sirvan como nuestros datos de entrenamiento. En cada experimento, aprendemos un $f$  diferente vinculado a los datos de entrenamiento de ese experimento. Si para un punto de prueba no visto $x$, evaluamos $\\hat{f}(x)$ para cada experimento, reuniremos 10,000 valores para $\\hat{f}(x)$. Hacemos esto para los disitintos modelos y generamos un histograma.\n"]},{"cell_type":"code","metadata":{"id":"8_BgGIXIOw7r"},"source":["R = 10000\n","#d_arr = [1, 2, 3, 5]\n","y_hat_test = np.zeros((len(d_arr), R))\n","\n","for r in range(R):\n","    n = int(.02 * N)\n","    idx = np.random.permutation(N)[:n]\n","    x_train, y_train = x[idx], y[idx]\n","\n","    for k in range(len(d_arr)):\n","        d = d_arr[k]\n","        w = np.polyfit(x_train, y_train, d)\n","        y_hat_test[k, r] = f_hat(x_test, w)\n","\n","y_hat_test_mean = np.mean(y_hat_test, 1)\n","y_hat_test_std = np.std(y_hat_test, 1)\n","\n","fig, axs = plt.subplots(2, 1, sharex=True, sharey=True, figsize=(12, 6))\n","d_arr = [1, 2]\n","\n","for z in d_arr:\n","    k = z -1\n","    axs[k].hist(y_hat_test[k], density=True, color=colors[k], alpha=0.6,bins=30)\n","    xlim = axs[k].get_xlim()\n","    axs[k].plot([f(x_test), f(x_test)], [0, 1], 'r', linewidth=3.0)\n","    axs[k].plot([y_hat_test_mean[k], y_hat_test_mean[k]], [0, 1], c='k', linewidth=3.0)\n","    axs[k].title.set_text('d = {}'.format(d_arr[k]))\n","    axs[k].legend([r'$f\\:(x_{test})$', r'$\\mathbb{E}\\:[\\hat{f}(x_{test})]$', r'$\\hat{f}\\:(x_{test})$'], fontsize=12)\n","\n","for k in range(2):\n","    x_range = np.linspace(xlim[0], xlim[1], 1000)\n","    axs[k].plot(x_range, stats.norm.pdf(x_range, y_hat_test_mean[k], y_hat_test_std[k]), color=colors[k], ls='--')\n","\n","plt.suptitle(r'Histograma de  $\\hat{f}(x_{test})$', size=12)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mOQx89NAOw7t"},"source":["Ahora consideremos 1,000 puntos de prueba y calculemos el MSE de prueba promedio (sobre estos puntos). También calculamos el sesgo cuadrado promedio (sobre estos 1,000 puntos de prueba) y la varianza promedio. Si hacemos esto para cinco modelos, desde el grado $d = 0$ (línea horizontal) hasta el grado $d = 4$, obtenemos la siguiente gráfica."]},{"cell_type":"code","metadata":{"id":"8XVDFkktOw7t"},"source":["# EL número de la muestra total\n","R = 10000\n","N = 1000\n","x_max = 3\n","# rango utilizado en la construcción del modelo\n","n = int(.02 * N)\n","n_test = 1000\n","d_arr = np.arange(5)\n","# Generamos nuestro dataset.\n","x_test = x_max + np.random.rand(n_test) - .5\n","\n","epsilon = sigma_epsilon * np.random.randn(n_test)\n","# Modelamos con un ruido.\n","y_test = f(x_test) + epsilon\n","\n","train_squared_error = np.zeros((len(d_arr), R))\n","y_hat_test = np.zeros((len(d_arr), R, n_test))\n","for r in range(R):\n","    n = int(.02 * N)\n","    idx = np.random.permutation(N)[:n]\n","    x_train, y_train = x[idx], y[idx]\n","    for k in range(len(d_arr)):\n","        d = d_arr[k]\n","        w = np.polyfit(x_train, y_train, d)\n","        train_squared_error[k, r] = np.mean((y_train - f_hat(x_train, w)) ** 2)\n","        y_hat_test[k, r, :] = f_hat(x_test, w)\n","\n","test_squared_error = np.mean((y_hat_test - y_test) ** 2, 1)\n","bias_squared = (np.mean(y_hat_test, 1) - f(x_test)) ** 2\n","var_y_hat_test = np.var(y_hat_test, 1)\n","\n","plt.figure(figsize=(12, 8))\n","plt.plot(d_arr, np.mean(test_squared_error, 1), 'g', linewidth=3.0)\n","plt.plot(d_arr, np.mean(train_squared_error, 1), 'k', linewidth=3.0)\n","plt.plot(d_arr, np.mean(bias_squared, 1), 'm--')\n","plt.plot(d_arr, np.mean(var_y_hat_test, 1), 'b--')\n","plt.plot(d_arr, (sigma_epsilon ** 2) * np.ones_like(d_arr), 'r--')\n","# plt.plot(d_arr, np.mean(bias_squared + var_y_hat_test + sigma_epsilon ** 2, 1), 'm--')\n","plt.xticks(d_arr)\n","plt.xlabel('d', size=12)\n","plt.legend(['error de test', 'error de entrenamiento', r'bias al cuadrado: $(\\mathbb{E}[\\hat{f}(x)] - f(x))^2$',\n","            r'$variance (\\hat{f}(x))$', r' error irreducible: $\\sigma_\\epsilon^2$'], loc='upper center', fontsize=12)\n","plt.ylim([0, 12])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fU593MohOw7v"},"source":["$$\n","\\large \\color{green}{\\mathbb{E}[\\mathbb{E}[(y - \\hat{f}(x))^2]]} = \\color{magenta}{\\mathbb{E}[bias[\\hat{f}(x)]^2]} + \\color{blue}{\\mathbb{E}[var(\\hat{f}(x))]} + \\color{red}{\\sigma_e^2}\n","$$"]},{"cell_type":"code","metadata":{"id":"ArPFu12JOw7w"},"source":["    plt.figure(figsize=(12, 6))\n","    plt.scatter(x, y, alpha=0.5) # Los puntos con ruido\n","    plt.scatter(x_test, y_test, c='r', alpha=0.5) # El punto test\n","    plt.xlabel('x', size=12)\n","    plt.ylabel('y', size=12)\n","    plt.xticks(np.arange(-x_max, x_max + 1))\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Comprender el _bias_ y el _variance_ es fundamental para entender el comportamiento de los modelos de predicción, pero en general lo que realmente importa es el _error general_, no la descomposición específica. El punto ideal para cualquier modelo es el nivel de complejidad en el que el aumento en el _bias_ es equivalente a la reducción en el _variance_.\n","\n","**RECORDAR**:\n","No hay escapatoria a la relación entre _bias_ y _variance_ en Machine Learning:\n","* Aumentar el sesgo disminuirá la varianza.\n","* Aumentar la varianza disminuirá el sesgo.\n","\n","Existe una relación de intercambio entre estas dos preocupaciones, el sesgo y la varianza proporcionan las herramientas para comprender el comportamiento de los algoritmos de aprendizaje automático en la búsqueda del mejor rendimiento predictivo.\n","\n","<img src='https://drive.google.com/uc?export=view&id=19hksq8XUsNcnPdIQSzhy6oma1N7ko69I' width=\"400\" align=\"center\" style=\"margin-right: 20px\">"],"metadata":{"id":"izzn3rvJEnn8"}}]}