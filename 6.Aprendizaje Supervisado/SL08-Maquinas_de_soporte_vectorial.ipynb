{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"xEiocYIB9uYi"},"source":["# **Aprendizaje supervisado**\n","# SL08. Máquinas de Soporte Vectorial (SVM)"]},{"cell_type":"markdown","metadata":{"id":"NS2K3V1-mI-T"},"source":["## <font color='blue'>**Máquinas de Soporte Vectorial (SVM)**</font>\n","\n","\n","Las **máquinas de soporte vectorial (SVM)** son una clase particularmente poderosa y flexible de **algoritmos supervisados** tanto para clasificación como para regresión."]},{"cell_type":"code","metadata":{"id":"_Zk2w-yMm8ZR"},"source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy import stats\n","\n","# use seaborn plotting defaults\n","import seaborn as sns; sns.set()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fmWyOerLm_tu"},"source":["SVM corresponde a un tipo de clasificación **discriminativa**, que es la que busca modelar cada clase, encontrando una línea o curva (en dos dimensiones) o un plano (en N-dimensiones) que divide las clases entre sí.\n","\n","Como ejemplo de esto, considere el caso simple de una tarea de clasificación, en la que las dos clases de puntos están bien separadas:"]},{"cell_type":"code","metadata":{"id":"c99RvBJwpMyM"},"source":["from sklearn.datasets import make_blobs\n","X, y = make_blobs(n_samples=50, centers=2,\n","                  random_state=0, cluster_std=0.60)\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X, y = make_blobs(n_samples=50, centers=2,\n","                  random_state=0, cluster_std=1.6) # Modificamos cluster_std\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"],"metadata":{"id":"iRw1hdVMxnxn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HTdhr5x4pWIo"},"source":["Un clasificador discriminativo lineal intentaría trazar una línea recta que separe los dos conjuntos de datos y, por lo tanto, crearía un modelo de clasificación. Para datos bidimensionales como el que se muestra aquí, esta es una tarea que podríamos hacer a mano. Pero inmediatamente vemos un problema: **¡hay más de una posible línea divisoria que puede discriminar perfectamente entre las dos clases!**\n","\n","Podemos dibujarlos de la siguiente manera:"]},{"cell_type":"code","metadata":{"id":"6gsbFy07ph0d"},"source":["xfit = np.linspace(-1, 3.5)\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n","plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\n","\n","for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n","    plt.plot(xfit, m * xfit + b, '-k')\n","\n","plt.xlim(-1, 3.5);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R3DkcvWtptTo"},"source":["Se trata de tres separadores muy diferentes que, sin embargo, discriminan perfectamente entre estas muestras. Dependiendo de cuál se elija, a un nuevo punto de datos (por ejemplo, el marcado con la \"X\" en este gráfico) se le asignará una etiqueta diferente. Evidentemente, nuestra simple intuición de \"trazar una línea divisoria entre clases\" no es suficiente, y necesitamos pensar un poco más."]},{"cell_type":"markdown","metadata":{"id":"h73IPlKQp1f9"},"source":["## Maximizando el *Margen*\n","\n","Las máquinas de soporte vectorial ofrecen una forma mejorada de solucionar este problema.\n","\n","La idea es la siguiente: en lugar de simplemente dibujar una línea de ancho cero entre las clases, podemos dibujar alrededor de cada línea un margen de cierto ancho, hasta el punto más cercano. A continuación, se muestra un ejemplo de cómo podría verse esto:"]},{"cell_type":"code","metadata":{"id":"jc9h0TdQqEa7"},"source":["xfit = np.linspace(-1, 3.5)\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n","\n","for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n","    yfit = m * xfit + b\n","    plt.plot(xfit, yfit, '-k')\n","    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n","                     color='#AAAAAA', alpha=0.4)\n","\n","plt.xlim(-1, 3.5);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"isZVCvQwrJzn"},"source":["En las SVM, la línea que **maximiza este margen** es la que elegiremos como modelo óptimo. Las SVM son un ejemplo de estimador de margen máximo."]},{"cell_type":"markdown","metadata":{"id":"tFjiA6kkrbu_"},"source":["## Ajuste de una máquina de vectores de soporte\n","\n","Veamos el resultado de un ajuste real a los datos del ejemplo: utilizaremos el clasificador de vectores de soporte de Scikit-Learn para entrenar un modelo SVM sobre estos datos. Por el momento, usaremos un kernel lineal y estableceremos el parámetro C en un número muy grande (discutiremos el significado de estos con más profundidad momentáneamente)."]},{"cell_type":"code","metadata":{"id":"2IExxxyGrJH1"},"source":["from sklearn.svm import SVC # \"Support vector classifier\"\n","model = SVC(kernel='linear', C=1E10)\n","model.fit(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i4YPslmHrs3i"},"source":["Para visualizar mejor lo que está sucediendo aquí, creemos una función que permita trazar los límites de decisión del SVM:"]},{"cell_type":"code","metadata":{"id":"I-ml9W80rwux"},"source":["def plot_svc_decision_function(model, ax=None, plot_support=True):\n","    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n","    if ax is None:\n","        ax = plt.gca()\n","    xlim = ax.get_xlim()\n","    ylim = ax.get_ylim()\n","\n","    # create grid to evaluate model\n","    x = np.linspace(xlim[0], xlim[1], 30)\n","    y = np.linspace(ylim[0], ylim[1], 30)\n","    Y, X = np.meshgrid(y, x)\n","    xy = np.vstack([X.ravel(), Y.ravel()]).T\n","    P = model.decision_function(xy).reshape(X.shape)\n","\n","    # plot decision boundary and margins\n","    ax.contour(X, Y, P, colors='k',\n","               levels=[-1, 0, 1], alpha=0.5,\n","               linestyles=['--', '-', '--'])\n","\n","    # plot support vectors\n","    if plot_support:\n","        ax.scatter(model.support_vectors_[:, 0],\n","                   model.support_vectors_[:, 1],\n","                   s=300, linewidth=1, facecolors='none');\n","    ax.set_xlim(xlim)\n","    ax.set_ylim(ylim)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VrxcV3gTry12"},"source":["plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n","plot_svc_decision_function(model);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AuCrSGxfsFuS"},"source":["Esta es la **línea divisoria que maximiza el margen entre los dos conjuntos de puntos**. Observe que algunos de los puntos de entrenamiento solo tocan el margen. Estos puntos son los elementos fundamentales de este ajuste y se conocen como vectores de soporte, y dan nombre al algoritmo. En Scikit-Learn, los valores de estos puntos se almacenan en el atributo *support_vectors_* del clasificador:"]},{"cell_type":"code","metadata":{"id":"hposjdxTsSRK"},"source":["model.support_vectors_"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zt8jIqvUs4hE"},"source":["Una clave del éxito de este clasificador es que para el ajuste, solo importa la posición de los vectores de soporte; ¡Cualquier punto más alejado del margen que esté en el lado correcto no modifica el ajuste! Técnicamente, esto se debe a que estos puntos no contribuyen a la función de pérdida utilizada para ajustar el modelo, por lo que su posición y número no importan siempre que no crucen el margen.\n","\n","Podemos ver esto, por ejemplo, si graficamos el modelo entrenado con los primeros 60 puntos y luego con los primeros 120 puntos de este conjunto de datos:"]},{"cell_type":"code","metadata":{"id":"UeudbnAUtBbg"},"source":["def plot_svm(N=10, ax=None):\n","    X, y = make_blobs(n_samples=200, centers=2,\n","                      random_state=0, cluster_std=0.60)\n","    X = X[:N]\n","    y = y[:N]\n","    model = SVC(kernel='linear', C=1E10)\n","    model.fit(X, y)\n","\n","    ax = ax or plt.gca()\n","    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n","    ax.set_xlim(-1, 4)\n","    ax.set_ylim(-1, 6)\n","    plot_svc_decision_function(model, ax)\n","\n","fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n","fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n","for axi, N in zip(ax, [60, 120]):\n","    plot_svm(N, axi)\n","    axi.set_title('N = {0}'.format(N))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L_-j2e4stlqI"},"source":["En el gráfico de la izquierda vemos el modelo y los vectores de soporte para 60 puntos de entrenamiento. En el gráfico de la derecha, hemos duplicado el número de puntos de entrenamiento, pero el modelo no ha cambiado: los tres vectores de soporte del gráfico izquierdo siguen siendo los vectores de soporte del gráfico derecho. Esta insensibilidad al comportamiento exacto de los puntos distantes es uno de los puntos fuertes del modelo SVM."]},{"cell_type":"code","metadata":{"id":"zowjv3HZunFa"},"source":["from ipywidgets import interact, fixed\n","interact(plot_svm, N=[10, 50, 100, 200, 1000], ax=fixed(None));"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d6nGG23H-2ZH"},"source":["## <font color='green'>Actividad 1</font>\n","\n","Entrene una SVM para clasificar correctamente los siguientes datos:"]},{"cell_type":"code","metadata":{"id":"R3oaWdR__CKC"},"source":["from sklearn.datasets import make_blobs\n","X, y = make_blobs(100, 2, centers=2, random_state=2, cluster_std=1)\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s2tEvK49_Xvq"},"source":["Ajuste la SVM con un kernel lineal."]},{"cell_type":"code","metadata":{"id":"L3FW29lm_Yah"},"source":["# Tú codigo aquí"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NgqWfDoi_bZE"},"source":["Grafique los puntos y el plano separador."]},{"cell_type":"code","metadata":{"id":"t0Y5GzgP_epL"},"source":["# Tú codigo aquí"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l2t2r8DT_lXc"},"source":["<font color='green'>Fin Actividad 1</font>"]},{"cell_type":"markdown","metadata":{"id":"uVB1Jq4fuzGp"},"source":["### Kernel SVM\n","\n","SVM se vuelve extremadamente poderoso cuando se combina con **kernels**.\n","\n","La manera más simple de realizar la separación es mediante una línea recta, un plano o un hiperplano N-dimensional.\n","\n","Desafortunadamente los universos a estudiar no se suelen presentar en casos idílicos de dos dimensiones como en el ejemplo anterior, sino que un algoritmo SVM debe tratar con a) más de dos variables predictoras, b) curvas no lineales de separación, c) casos donde los conjuntos de datos no pueden ser completamente separados, d) clasificaciones en más de dos categorías.\n","\n","Debido a las limitaciones computacionales de las máquinas de aprendizaje lineal, éstas no pueden ser utilizadas en la mayoría de las aplicaciones del mundo real. **La representación por medio de funciones Kernel ofrece una solución a este problema**, proyectando la información a un espacio de características de mayor dimensión el cual aumenta la capacidad computacional de la máquinas de aprendizaje lineal. Es decir, **mapearemos el espacio de entradas X a un nuevo espacio de características de mayor dimensionalidad (Hilbert)**:\n","\n","Para motivar la necesidad de kernels, veamos algunos datos que no se pueden separar linealmente:"]},{"cell_type":"code","metadata":{"id":"SP8x1tS6wIC4"},"source":["from sklearn.datasets import make_circles\n","X, y = make_circles(100, factor=.1, noise=.1)\n","\n","clf = SVC(kernel='linear').fit(X, y)\n","\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n","plot_svc_decision_function(clf, plot_support=False);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PXQw4Zo8wPJd"},"source":["Está claro que ninguna discriminación lineal podrá separar estos datos. La solución es proyectar los datos en una dimensión superior de modo que un separador lineal sea suficiente. Por ejemplo, una proyección simple que podríamos usar sería calcular una función de base radial centrada en el grupo del medio:"]},{"cell_type":"code","metadata":{"id":"J_uWY0aqwhLZ"},"source":["# Transformamos los datos, elevándolos al cuadrado\n","# A esto se\n","r = np.exp(-(X ** 2).sum(1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X[:10]"],"metadata":{"id":"ofd-6WJw3sDQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.exp(X[:2] ** 2)"],"metadata":{"id":"RpZaraHH3-Wh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["r[:10]"],"metadata":{"id":"SSJfK6IU3uwB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.scatter(X[:20].sum(1), r[:20])\n","plt.scatter(X[:20].sum(1), X[:20].sum(1), color='r')\n"],"metadata":{"id":"o9PdmrGm1IrW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kF8E2uWBwku0"},"source":["Podemos visualizar esta dimensión de datos adicional utilizando un gráfico tridimensional:"]},{"cell_type":"code","metadata":{"id":"Nur4PJ_kwquz"},"source":["from mpl_toolkits import mplot3d\n","\n","def plot_3D(elev=30, azim=30, X=X, y=y):\n","    ax = plt.subplot(projection='3d')\n","    ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')\n","    ax.view_init(elev=elev, azim=azim)\n","    ax.set_xlabel('x')\n","    ax.set_ylabel('y')\n","    ax.set_zlabel('r')\n","\n","interact(plot_3D, elev=[-90, 30, -60, 90], azip=(-180, 180),\n","         X=fixed(X), y=fixed(y));"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gNyH0W8pw4SW"},"source":["Podemos ver que con esta dimensión adicional, los datos se vuelven trivialmente separables linealmente, dibujando un plano de separación en, digamos, r = 0,7.\n","\n","Aquí tuvimos que elegir y ajustar cuidadosamente nuestra proyección: si no hubiéramos centrado nuestra función de base radial en la ubicación correcta, no hubiéramos visto resultados tan limpios y linealmente separables. En general, la necesidad de hacer tal elección es un problema: nos gustaría de alguna manera encontrar automáticamente las mejores funciones de base para usar.\n","\n","Una estrategia para este fin es calcular una función base centrada en cada punto del conjunto de datos y dejar que el algoritmo SVM revise los resultados. Este tipo de transformación de función de base se conoce como transformación de kernel, ya que se basa en una relación de similitud (o kernel) entre cada par de puntos.\n","\n","Un problema potencial con esta estrategia, proyectar 𝑁 puntos en 𝑁 dimensiones, es que puede volverse muy intensiva en computación a medida que 𝑁 crece. Sin embargo, debido a un pequeño y prolijo procedimiento conocido como el truco del kernel, se puede hacer un ajuste en los datos transformados del kernel de forma implícita, es decir, ¡sin construir la representación 𝑁-dimensional completa de la proyección del kernel! Este truco del kernel está integrado en SVM y es una de las razones por las que el método es tan poderoso.\n","\n","En Scikit-Learn, podemos aplicar SVM kernelizado simplemente cambiando nuestro kernel lineal a un kernel RBF (función de base radial), usando el hiperparámetro del modelo de kernel:"]},{"cell_type":"code","metadata":{"id":"0Q0dh2n0x1pS"},"source":["clf = SVC(kernel='rbf', C=1E6)\n","clf.fit(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zM4tVqsUx5Na"},"source":["plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n","plot_svc_decision_function(clf)\n","plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n","            s=300, lw=1, facecolors='none');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"axk3XiA4yAsC"},"source":["Usando esta SVM kernelizada, el modelo es capaz de determinar un límite de decisión no lineal adecuado. Esta estrategia de transformación del kernel se usa a menudo en el aprendizaje automático para convertir métodos lineales rápidos en métodos no lineales rápidos, especialmente para modelos en los que se puede usar el truco del kernel."]},{"cell_type":"markdown","metadata":{"id":"zcuSPsKTyMz3"},"source":["### Tuning de SVM\n","\n","Hasta ahora, nuestra discusión se ha centrado en conjuntos de datos muy limpios, en los que existe un límite de decisión perfecto. Pero, ¿qué pasa si sus datos tienen cierta superposición? Por ejemplo, puede tener datos como este:"]},{"cell_type":"code","metadata":{"id":"-I3n-EeIyQDS"},"source":["X, y = make_blobs(n_samples=100, centers=2,\n","                  random_state=0, cluster_std=1.2)\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"trvrGjmfyVhy"},"source":["Para manejar este caso, la implementación de SVM tiene un pequeño factor de modificación que \"suaviza\" el margen: es decir, permite que algunos de los puntos se introduzcan en el margen si eso permite un mejor ajuste. La \"rigidez\" del margen se controla mediante un parámetro de ajuste, más a menudo conocido como 𝐶 (regularización). Para 𝐶 muy grande, el margen es más estricto y los puntos no pueden estar en él. Para 𝐶 más pequeño, el margen es más suave y puede crecer para abarcar algunos puntos.\n","\n","El gráfico que se muestra a continuación ofrece una imagen visual de cómo un parámetro 𝐶 cambiante afecta el ajuste final, a través de la regularización del margen:"]},{"cell_type":"code","metadata":{"id":"Q2kaLGyBykfa"},"source":["X, y = make_blobs(n_samples=100, centers=2,\n","                  random_state=0, cluster_std=0.8)\n","\n","fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n","fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n","\n","for axi, C in zip(ax, [10.0, 0.1]):\n","    model = SVC(kernel='linear', C=C).fit(X, y)\n","    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n","    plot_svc_decision_function(model, axi)\n","    axi.scatter(model.support_vectors_[:, 0],\n","                model.support_vectors_[:, 1],\n","                s=300, lw=1, facecolors='none');\n","    axi.set_title('C = {0:.1f}'.format(C), size=14)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jn95cWqvytRb"},"source":["El valor óptimo del parámetro 𝐶 dependerá de su conjunto de datos y debe ajustarse mediante validación cruzada o un procedimiento similar."]},{"cell_type":"markdown","metadata":{"id":"RTs3cbElArHy"},"source":["## <font color='green'>Actividad 2</font>\n","\n","Genere un modelo de SVM con kernel RBF, grafique el resultado y muestre el limite de decisión y su margen.\n","\n","Utilice los siguientes datos:"]},{"cell_type":"code","metadata":{"id":"irSb0ZNeAzEF"},"source":["from sklearn.datasets import make_circles\n","X, y = make_circles(100, factor=.3, noise=.2)\n","\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"67nc7gMIBE2-"},"source":["# Tú codigo aquí"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mD08qBiJBdHN"},"source":["<font color='green'>Fin Actividad 2</font>"]},{"cell_type":"markdown","metadata":{"id":"T0lNp3KgyzVL"},"source":["## <font color='blue'>**Ejemplo: Face Recognition**</font>\n","\n","Como ejemplo de aplicación de SVM, tenemos el problema del reconocimiento facial. Usaremos el conjunto de datos *Labeled Faces in the Wild dataset*, que consta de varios miles de fotos recopiladas de varias figuras públicas. Un buscador para el conjunto de datos está integrado en Scikit-Learn:"]},{"cell_type":"code","metadata":{"id":"JNxLSzNkzNkZ"},"source":["from sklearn.datasets import fetch_lfw_people\n","faces = fetch_lfw_people(min_faces_per_person=60)\n","print(faces.target_names)\n","print(faces.images.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l7PlEDLzzdrL"},"source":["fig, ax = plt.subplots(3, 5)\n","for i, axi in enumerate(ax.flat):\n","    axi.imshow(faces.images[i], cmap='bone')\n","    axi.set(xticks=[], yticks=[],\n","            xlabel=faces.target_names[faces.target[i]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w9CISB_uzjln"},"source":["Cada imagen es de (62 × 47) píxeles. Podríamos proceder simplemente usando cada valor de píxel como una característica, pero a menudo es más efectivo usar algún tipo de preprocesador para extraer características más significativas; aquí usaremos un análisis de componentes principales para extraer 150 componentes fundamentales para alimentar a nuestro clasificador SVM."]},{"cell_type":"code","metadata":{"id":"2t6O2FpOzyxv"},"source":["from sklearn.svm import SVC\n","from sklearn.decomposition import PCA\n","from sklearn.pipeline import make_pipeline\n","\n","# Tomamos las imágenes, las aplanamos a vectores de 62 x 47 = 2.914\n","# Aplicamos PCA para elegir las, por ejemplo, 150 con mayor información\n","pca = PCA(n_components=150, whiten=True, random_state=42)\n","svc = SVC(kernel='rbf', class_weight='balanced')\n","model = make_pipeline(pca, svc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a2Sx_oDt0OnM"},"source":["from sklearn.model_selection import train_test_split\n","Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target,\n","                                                random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["faces.target"],"metadata":{"id":"f4kyEu3eGjen"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["faces.target_names"],"metadata":{"id":"vzhh8XhyGoFq"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GbBqdMta0eYV"},"source":["from sklearn.model_selection import GridSearchCV\n","param_grid = {'svc__C': [1, 5, 10, 50],\n","              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\n","\n","# Grid search nos ayuda a probar con distintos parámetros\n","grid = GridSearchCV(model, param_grid)\n","\n","%time grid.fit(Xtrain, ytrain)\n","print(grid.best_params_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dvUYpb1z1GUR"},"source":["model = grid.best_estimator_\n","yfit = model.predict(Xtest)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pLA4qkvg1JhF"},"source":["fig, ax = plt.subplots(4, 6)\n","\n","for i, axi in enumerate(ax.flat):\n","    axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')\n","    axi.set(xticks=[], yticks=[])\n","    axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],\n","                   color='black' if yfit[i] == ytest[i] else 'red')\n","fig.suptitle('Nombres Predicho; Etiquetas incorrectas en rojo', size=14);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vi1KsSJ21aSY"},"source":["from sklearn.metrics import classification_report\n","print(classification_report(ytest, yfit,\n","                            target_names=faces.target_names))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Axhnqy731fuR"},"source":["from sklearn.metrics import confusion_matrix\n","mat = confusion_matrix(ytest, yfit)\n","sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n","            xticklabels=faces.target_names,\n","            yticklabels=faces.target_names)\n","plt.xlabel('true label')\n","plt.ylabel('predicted label');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tq0xBrQv1rpE"},"source":["## <font color='blue'>**Resumen de las SVM**</font>\n","\n","Este método de clasificación es muy poderoso por varias razones:\n","\n","* Su dependencia de relativamente pocos vectores de soporte significa que son modelos muy compactos y ocupan muy poca memoria.\n","\n","* Una vez que se entrena el modelo, la fase de predicción es muy rápida.\n","\n","* Debido a que solo se ven afectados por puntos cercanos al margen, funcionan bien con datos de alta dimensión, incluso datos con más dimensiones que muestras.\n","\n","* Su integración con los métodos del kernel los hace muy versátiles, capaces de adaptarse a muchos tipos de datos.\n","\n","Sin embargo, las SVM también tienen varias desventajas:\n","\n","* La escala con el número de muestras 𝑁 es $O[𝑁^{3}]$ en el peor de los casos, o $O[𝑁^{2}]$ para implementaciones eficientes. Para un gran número de muestras, el costo computacional puede resultar prohibitivo.\n","\n","* Los resultados dependen en gran medida de una elección adecuada para el parámetro de regularización 𝐶. Este debe elegirse cuidadosamente a través de la validación cruzada, que puede ser costosa a medida que los conjuntos de datos aumentan de tamaño.\n","\n","* Los resultados no tienen una interpretación probabilística directa. Esto se puede estimar mediante una validación cruzada interna, pero esta estimación adicional es costosa."]}]}