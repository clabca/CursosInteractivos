{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"xEiocYIB9uYi"},"source":["# **Aprendizaje supervisado**\n","# SL08. M치quinas de Soporte Vectorial (SVM)"]},{"cell_type":"markdown","metadata":{"id":"NS2K3V1-mI-T"},"source":["## <font color='blue'>**M치quinas de Soporte Vectorial (SVM)**</font>\n","\n","\n","Las **m치quinas de soporte vectorial (SVM)** son una clase particularmente poderosa y flexible de **algoritmos supervisados** tanto para clasificaci칩n como para regresi칩n."]},{"cell_type":"code","metadata":{"id":"_Zk2w-yMm8ZR"},"source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy import stats\n","\n","# use seaborn plotting defaults\n","import seaborn as sns; sns.set()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fmWyOerLm_tu"},"source":["SVM corresponde a un tipo de clasificaci칩n **discriminativa**, que es la que busca modelar cada clase, encontrando una l칤nea o curva (en dos dimensiones) o un plano (en N-dimensiones) que divide las clases entre s칤.\n","\n","Como ejemplo de esto, considere el caso simple de una tarea de clasificaci칩n, en la que las dos clases de puntos est치n bien separadas:"]},{"cell_type":"code","metadata":{"id":"c99RvBJwpMyM"},"source":["from sklearn.datasets import make_blobs\n","X, y = make_blobs(n_samples=50, centers=2,\n","                  random_state=0, cluster_std=0.60)\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X, y = make_blobs(n_samples=50, centers=2,\n","                  random_state=0, cluster_std=1.6) # Modificamos cluster_std\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"],"metadata":{"id":"iRw1hdVMxnxn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HTdhr5x4pWIo"},"source":["Un clasificador discriminativo lineal intentar칤a trazar una l칤nea recta que separe los dos conjuntos de datos y, por lo tanto, crear칤a un modelo de clasificaci칩n. Para datos bidimensionales como el que se muestra aqu칤, esta es una tarea que podr칤amos hacer a mano. Pero inmediatamente vemos un problema: **춰hay m치s de una posible l칤nea divisoria que puede discriminar perfectamente entre las dos clases!**\n","\n","Podemos dibujarlos de la siguiente manera:"]},{"cell_type":"code","metadata":{"id":"6gsbFy07ph0d"},"source":["xfit = np.linspace(-1, 3.5)\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n","plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\n","\n","for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n","    plt.plot(xfit, m * xfit + b, '-k')\n","\n","plt.xlim(-1, 3.5);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R3DkcvWtptTo"},"source":["Se trata de tres separadores muy diferentes que, sin embargo, discriminan perfectamente entre estas muestras. Dependiendo de cu치l se elija, a un nuevo punto de datos (por ejemplo, el marcado con la \"X\" en este gr치fico) se le asignar치 una etiqueta diferente. Evidentemente, nuestra simple intuici칩n de \"trazar una l칤nea divisoria entre clases\" no es suficiente, y necesitamos pensar un poco m치s."]},{"cell_type":"markdown","metadata":{"id":"h73IPlKQp1f9"},"source":["## Maximizando el *Margen*\n","\n","Las m치quinas de soporte vectorial ofrecen una forma mejorada de solucionar este problema.\n","\n","La idea es la siguiente: en lugar de simplemente dibujar una l칤nea de ancho cero entre las clases, podemos dibujar alrededor de cada l칤nea un margen de cierto ancho, hasta el punto m치s cercano. A continuaci칩n, se muestra un ejemplo de c칩mo podr칤a verse esto:"]},{"cell_type":"code","metadata":{"id":"jc9h0TdQqEa7"},"source":["xfit = np.linspace(-1, 3.5)\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n","\n","for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n","    yfit = m * xfit + b\n","    plt.plot(xfit, yfit, '-k')\n","    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n","                     color='#AAAAAA', alpha=0.4)\n","\n","plt.xlim(-1, 3.5);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"isZVCvQwrJzn"},"source":["En las SVM, la l칤nea que **maximiza este margen** es la que elegiremos como modelo 칩ptimo. Las SVM son un ejemplo de estimador de margen m치ximo."]},{"cell_type":"markdown","metadata":{"id":"tFjiA6kkrbu_"},"source":["## Ajuste de una m치quina de vectores de soporte\n","\n","Veamos el resultado de un ajuste real a los datos del ejemplo: utilizaremos el clasificador de vectores de soporte de Scikit-Learn para entrenar un modelo SVM sobre estos datos. Por el momento, usaremos un kernel lineal y estableceremos el par치metro C en un n칰mero muy grande (discutiremos el significado de estos con m치s profundidad moment치neamente)."]},{"cell_type":"code","metadata":{"id":"2IExxxyGrJH1"},"source":["from sklearn.svm import SVC # \"Support vector classifier\"\n","model = SVC(kernel='linear', C=1E10)\n","model.fit(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i4YPslmHrs3i"},"source":["Para visualizar mejor lo que est치 sucediendo aqu칤, creemos una funci칩n que permita trazar los l칤mites de decisi칩n del SVM:"]},{"cell_type":"code","metadata":{"id":"I-ml9W80rwux"},"source":["def plot_svc_decision_function(model, ax=None, plot_support=True):\n","    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n","    if ax is None:\n","        ax = plt.gca()\n","    xlim = ax.get_xlim()\n","    ylim = ax.get_ylim()\n","\n","    # create grid to evaluate model\n","    x = np.linspace(xlim[0], xlim[1], 30)\n","    y = np.linspace(ylim[0], ylim[1], 30)\n","    Y, X = np.meshgrid(y, x)\n","    xy = np.vstack([X.ravel(), Y.ravel()]).T\n","    P = model.decision_function(xy).reshape(X.shape)\n","\n","    # plot decision boundary and margins\n","    ax.contour(X, Y, P, colors='k',\n","               levels=[-1, 0, 1], alpha=0.5,\n","               linestyles=['--', '-', '--'])\n","\n","    # plot support vectors\n","    if plot_support:\n","        ax.scatter(model.support_vectors_[:, 0],\n","                   model.support_vectors_[:, 1],\n","                   s=300, linewidth=1, facecolors='none');\n","    ax.set_xlim(xlim)\n","    ax.set_ylim(ylim)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VrxcV3gTry12"},"source":["plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n","plot_svc_decision_function(model);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AuCrSGxfsFuS"},"source":["Esta es la **l칤nea divisoria que maximiza el margen entre los dos conjuntos de puntos**. Observe que algunos de los puntos de entrenamiento solo tocan el margen. Estos puntos son los elementos fundamentales de este ajuste y se conocen como vectores de soporte, y dan nombre al algoritmo. En Scikit-Learn, los valores de estos puntos se almacenan en el atributo *support_vectors_* del clasificador:"]},{"cell_type":"code","metadata":{"id":"hposjdxTsSRK"},"source":["model.support_vectors_"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zt8jIqvUs4hE"},"source":["Una clave del 칠xito de este clasificador es que para el ajuste, solo importa la posici칩n de los vectores de soporte; 춰Cualquier punto m치s alejado del margen que est칠 en el lado correcto no modifica el ajuste! T칠cnicamente, esto se debe a que estos puntos no contribuyen a la funci칩n de p칠rdida utilizada para ajustar el modelo, por lo que su posici칩n y n칰mero no importan siempre que no crucen el margen.\n","\n","Podemos ver esto, por ejemplo, si graficamos el modelo entrenado con los primeros 60 puntos y luego con los primeros 120 puntos de este conjunto de datos:"]},{"cell_type":"code","metadata":{"id":"UeudbnAUtBbg"},"source":["def plot_svm(N=10, ax=None):\n","    X, y = make_blobs(n_samples=200, centers=2,\n","                      random_state=0, cluster_std=0.60)\n","    X = X[:N]\n","    y = y[:N]\n","    model = SVC(kernel='linear', C=1E10)\n","    model.fit(X, y)\n","\n","    ax = ax or plt.gca()\n","    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n","    ax.set_xlim(-1, 4)\n","    ax.set_ylim(-1, 6)\n","    plot_svc_decision_function(model, ax)\n","\n","fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n","fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n","for axi, N in zip(ax, [60, 120]):\n","    plot_svm(N, axi)\n","    axi.set_title('N = {0}'.format(N))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L_-j2e4stlqI"},"source":["En el gr치fico de la izquierda vemos el modelo y los vectores de soporte para 60 puntos de entrenamiento. En el gr치fico de la derecha, hemos duplicado el n칰mero de puntos de entrenamiento, pero el modelo no ha cambiado: los tres vectores de soporte del gr치fico izquierdo siguen siendo los vectores de soporte del gr치fico derecho. Esta insensibilidad al comportamiento exacto de los puntos distantes es uno de los puntos fuertes del modelo SVM."]},{"cell_type":"code","metadata":{"id":"zowjv3HZunFa"},"source":["from ipywidgets import interact, fixed\n","interact(plot_svm, N=[10, 50, 100, 200, 1000], ax=fixed(None));"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d6nGG23H-2ZH"},"source":["## <font color='green'>Actividad 1</font>\n","\n","Entrene una SVM para clasificar correctamente los siguientes datos:"]},{"cell_type":"code","metadata":{"id":"R3oaWdR__CKC"},"source":["from sklearn.datasets import make_blobs\n","X, y = make_blobs(100, 2, centers=2, random_state=2, cluster_std=1)\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s2tEvK49_Xvq"},"source":["Ajuste la SVM con un kernel lineal."]},{"cell_type":"code","metadata":{"id":"L3FW29lm_Yah"},"source":["# T칰 codigo aqu칤"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NgqWfDoi_bZE"},"source":["Grafique los puntos y el plano separador."]},{"cell_type":"code","metadata":{"id":"t0Y5GzgP_epL"},"source":["# T칰 codigo aqu칤"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l2t2r8DT_lXc"},"source":["<font color='green'>Fin Actividad 1</font>"]},{"cell_type":"markdown","metadata":{"id":"uVB1Jq4fuzGp"},"source":["### Kernel SVM\n","\n","SVM se vuelve extremadamente poderoso cuando se combina con **kernels**.\n","\n","La manera m치s simple de realizar la separaci칩n es mediante una l칤nea recta, un plano o un hiperplano N-dimensional.\n","\n","Desafortunadamente los universos a estudiar no se suelen presentar en casos id칤licos de dos dimensiones como en el ejemplo anterior, sino que un algoritmo SVM debe tratar con a) m치s de dos variables predictoras, b) curvas no lineales de separaci칩n, c) casos donde los conjuntos de datos no pueden ser completamente separados, d) clasificaciones en m치s de dos categor칤as.\n","\n","Debido a las limitaciones computacionales de las m치quinas de aprendizaje lineal, 칠stas no pueden ser utilizadas en la mayor칤a de las aplicaciones del mundo real. **La representaci칩n por medio de funciones Kernel ofrece una soluci칩n a este problema**, proyectando la informaci칩n a un espacio de caracter칤sticas de mayor dimensi칩n el cual aumenta la capacidad computacional de la m치quinas de aprendizaje lineal. Es decir, **mapearemos el espacio de entradas X a un nuevo espacio de caracter칤sticas de mayor dimensionalidad (Hilbert)**:\n","\n","Para motivar la necesidad de kernels, veamos algunos datos que no se pueden separar linealmente:"]},{"cell_type":"code","metadata":{"id":"SP8x1tS6wIC4"},"source":["from sklearn.datasets import make_circles\n","X, y = make_circles(100, factor=.1, noise=.1)\n","\n","clf = SVC(kernel='linear').fit(X, y)\n","\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n","plot_svc_decision_function(clf, plot_support=False);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PXQw4Zo8wPJd"},"source":["Est치 claro que ninguna discriminaci칩n lineal podr치 separar estos datos. La soluci칩n es proyectar los datos en una dimensi칩n superior de modo que un separador lineal sea suficiente. Por ejemplo, una proyecci칩n simple que podr칤amos usar ser칤a calcular una funci칩n de base radial centrada en el grupo del medio:"]},{"cell_type":"code","metadata":{"id":"J_uWY0aqwhLZ"},"source":["# Transformamos los datos, elev치ndolos al cuadrado\n","# A esto se\n","r = np.exp(-(X ** 2).sum(1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X[:10]"],"metadata":{"id":"ofd-6WJw3sDQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.exp(X[:2] ** 2)"],"metadata":{"id":"RpZaraHH3-Wh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["r[:10]"],"metadata":{"id":"SSJfK6IU3uwB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.scatter(X[:20].sum(1), r[:20])\n","plt.scatter(X[:20].sum(1), X[:20].sum(1), color='r')\n"],"metadata":{"id":"o9PdmrGm1IrW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kF8E2uWBwku0"},"source":["Podemos visualizar esta dimensi칩n de datos adicional utilizando un gr치fico tridimensional:"]},{"cell_type":"code","metadata":{"id":"Nur4PJ_kwquz"},"source":["from mpl_toolkits import mplot3d\n","\n","def plot_3D(elev=30, azim=30, X=X, y=y):\n","    ax = plt.subplot(projection='3d')\n","    ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')\n","    ax.view_init(elev=elev, azim=azim)\n","    ax.set_xlabel('x')\n","    ax.set_ylabel('y')\n","    ax.set_zlabel('r')\n","\n","interact(plot_3D, elev=[-90, 30, -60, 90], azip=(-180, 180),\n","         X=fixed(X), y=fixed(y));"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gNyH0W8pw4SW"},"source":["Podemos ver que con esta dimensi칩n adicional, los datos se vuelven trivialmente separables linealmente, dibujando un plano de separaci칩n en, digamos, r = 0,7.\n","\n","Aqu칤 tuvimos que elegir y ajustar cuidadosamente nuestra proyecci칩n: si no hubi칠ramos centrado nuestra funci칩n de base radial en la ubicaci칩n correcta, no hubi칠ramos visto resultados tan limpios y linealmente separables. En general, la necesidad de hacer tal elecci칩n es un problema: nos gustar칤a de alguna manera encontrar autom치ticamente las mejores funciones de base para usar.\n","\n","Una estrategia para este fin es calcular una funci칩n base centrada en cada punto del conjunto de datos y dejar que el algoritmo SVM revise los resultados. Este tipo de transformaci칩n de funci칩n de base se conoce como transformaci칩n de kernel, ya que se basa en una relaci칩n de similitud (o kernel) entre cada par de puntos.\n","\n","Un problema potencial con esta estrategia, proyectar 洧녜 puntos en 洧녜 dimensiones, es que puede volverse muy intensiva en computaci칩n a medida que 洧녜 crece. Sin embargo, debido a un peque침o y prolijo procedimiento conocido como el truco del kernel, se puede hacer un ajuste en los datos transformados del kernel de forma impl칤cita, es decir, 춰sin construir la representaci칩n 洧녜-dimensional completa de la proyecci칩n del kernel! Este truco del kernel est치 integrado en SVM y es una de las razones por las que el m칠todo es tan poderoso.\n","\n","En Scikit-Learn, podemos aplicar SVM kernelizado simplemente cambiando nuestro kernel lineal a un kernel RBF (funci칩n de base radial), usando el hiperpar치metro del modelo de kernel:"]},{"cell_type":"code","metadata":{"id":"0Q0dh2n0x1pS"},"source":["clf = SVC(kernel='rbf', C=1E6)\n","clf.fit(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zM4tVqsUx5Na"},"source":["plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n","plot_svc_decision_function(clf)\n","plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n","            s=300, lw=1, facecolors='none');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"axk3XiA4yAsC"},"source":["Usando esta SVM kernelizada, el modelo es capaz de determinar un l칤mite de decisi칩n no lineal adecuado. Esta estrategia de transformaci칩n del kernel se usa a menudo en el aprendizaje autom치tico para convertir m칠todos lineales r치pidos en m칠todos no lineales r치pidos, especialmente para modelos en los que se puede usar el truco del kernel."]},{"cell_type":"markdown","metadata":{"id":"zcuSPsKTyMz3"},"source":["### Tuning de SVM\n","\n","Hasta ahora, nuestra discusi칩n se ha centrado en conjuntos de datos muy limpios, en los que existe un l칤mite de decisi칩n perfecto. Pero, 쯤u칠 pasa si sus datos tienen cierta superposici칩n? Por ejemplo, puede tener datos como este:"]},{"cell_type":"code","metadata":{"id":"-I3n-EeIyQDS"},"source":["X, y = make_blobs(n_samples=100, centers=2,\n","                  random_state=0, cluster_std=1.2)\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"trvrGjmfyVhy"},"source":["Para manejar este caso, la implementaci칩n de SVM tiene un peque침o factor de modificaci칩n que \"suaviza\" el margen: es decir, permite que algunos de los puntos se introduzcan en el margen si eso permite un mejor ajuste. La \"rigidez\" del margen se controla mediante un par치metro de ajuste, m치s a menudo conocido como 洧냤 (regularizaci칩n). Para 洧냤 muy grande, el margen es m치s estricto y los puntos no pueden estar en 칠l. Para 洧냤 m치s peque침o, el margen es m치s suave y puede crecer para abarcar algunos puntos.\n","\n","El gr치fico que se muestra a continuaci칩n ofrece una imagen visual de c칩mo un par치metro 洧냤 cambiante afecta el ajuste final, a trav칠s de la regularizaci칩n del margen:"]},{"cell_type":"code","metadata":{"id":"Q2kaLGyBykfa"},"source":["X, y = make_blobs(n_samples=100, centers=2,\n","                  random_state=0, cluster_std=0.8)\n","\n","fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n","fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n","\n","for axi, C in zip(ax, [10.0, 0.1]):\n","    model = SVC(kernel='linear', C=C).fit(X, y)\n","    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n","    plot_svc_decision_function(model, axi)\n","    axi.scatter(model.support_vectors_[:, 0],\n","                model.support_vectors_[:, 1],\n","                s=300, lw=1, facecolors='none');\n","    axi.set_title('C = {0:.1f}'.format(C), size=14)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jn95cWqvytRb"},"source":["El valor 칩ptimo del par치metro 洧냤 depender치 de su conjunto de datos y debe ajustarse mediante validaci칩n cruzada o un procedimiento similar."]},{"cell_type":"markdown","metadata":{"id":"RTs3cbElArHy"},"source":["## <font color='green'>Actividad 2</font>\n","\n","Genere un modelo de SVM con kernel RBF, grafique el resultado y muestre el limite de decisi칩n y su margen.\n","\n","Utilice los siguientes datos:"]},{"cell_type":"code","metadata":{"id":"irSb0ZNeAzEF"},"source":["from sklearn.datasets import make_circles\n","X, y = make_circles(100, factor=.3, noise=.2)\n","\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"67nc7gMIBE2-"},"source":["# T칰 codigo aqu칤"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mD08qBiJBdHN"},"source":["<font color='green'>Fin Actividad 2</font>"]},{"cell_type":"markdown","metadata":{"id":"T0lNp3KgyzVL"},"source":["## <font color='blue'>**Ejemplo: Face Recognition**</font>\n","\n","Como ejemplo de aplicaci칩n de SVM, tenemos el problema del reconocimiento facial. Usaremos el conjunto de datos *Labeled Faces in the Wild dataset*, que consta de varios miles de fotos recopiladas de varias figuras p칰blicas. Un buscador para el conjunto de datos est치 integrado en Scikit-Learn:"]},{"cell_type":"code","metadata":{"id":"JNxLSzNkzNkZ"},"source":["from sklearn.datasets import fetch_lfw_people\n","faces = fetch_lfw_people(min_faces_per_person=60)\n","print(faces.target_names)\n","print(faces.images.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l7PlEDLzzdrL"},"source":["fig, ax = plt.subplots(3, 5)\n","for i, axi in enumerate(ax.flat):\n","    axi.imshow(faces.images[i], cmap='bone')\n","    axi.set(xticks=[], yticks=[],\n","            xlabel=faces.target_names[faces.target[i]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w9CISB_uzjln"},"source":["Cada imagen es de (62 칑 47) p칤xeles. Podr칤amos proceder simplemente usando cada valor de p칤xel como una caracter칤stica, pero a menudo es m치s efectivo usar alg칰n tipo de preprocesador para extraer caracter칤sticas m치s significativas; aqu칤 usaremos un an치lisis de componentes principales para extraer 150 componentes fundamentales para alimentar a nuestro clasificador SVM."]},{"cell_type":"code","metadata":{"id":"2t6O2FpOzyxv"},"source":["from sklearn.svm import SVC\n","from sklearn.decomposition import PCA\n","from sklearn.pipeline import make_pipeline\n","\n","# Tomamos las im치genes, las aplanamos a vectores de 62 x 47 = 2.914\n","# Aplicamos PCA para elegir las, por ejemplo, 150 con mayor informaci칩n\n","pca = PCA(n_components=150, whiten=True, random_state=42)\n","svc = SVC(kernel='rbf', class_weight='balanced')\n","model = make_pipeline(pca, svc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a2Sx_oDt0OnM"},"source":["from sklearn.model_selection import train_test_split\n","Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target,\n","                                                random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["faces.target"],"metadata":{"id":"f4kyEu3eGjen"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["faces.target_names"],"metadata":{"id":"vzhh8XhyGoFq"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GbBqdMta0eYV"},"source":["from sklearn.model_selection import GridSearchCV\n","param_grid = {'svc__C': [1, 5, 10, 50],\n","              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\n","\n","# Grid search nos ayuda a probar con distintos par치metros\n","grid = GridSearchCV(model, param_grid)\n","\n","%time grid.fit(Xtrain, ytrain)\n","print(grid.best_params_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dvUYpb1z1GUR"},"source":["model = grid.best_estimator_\n","yfit = model.predict(Xtest)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pLA4qkvg1JhF"},"source":["fig, ax = plt.subplots(4, 6)\n","\n","for i, axi in enumerate(ax.flat):\n","    axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')\n","    axi.set(xticks=[], yticks=[])\n","    axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],\n","                   color='black' if yfit[i] == ytest[i] else 'red')\n","fig.suptitle('Nombres Predicho; Etiquetas incorrectas en rojo', size=14);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vi1KsSJ21aSY"},"source":["from sklearn.metrics import classification_report\n","print(classification_report(ytest, yfit,\n","                            target_names=faces.target_names))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Axhnqy731fuR"},"source":["from sklearn.metrics import confusion_matrix\n","mat = confusion_matrix(ytest, yfit)\n","sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n","            xticklabels=faces.target_names,\n","            yticklabels=faces.target_names)\n","plt.xlabel('true label')\n","plt.ylabel('predicted label');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tq0xBrQv1rpE"},"source":["## <font color='blue'>**Resumen de las SVM**</font>\n","\n","Este m칠todo de clasificaci칩n es muy poderoso por varias razones:\n","\n","* Su dependencia de relativamente pocos vectores de soporte significa que son modelos muy compactos y ocupan muy poca memoria.\n","\n","* Una vez que se entrena el modelo, la fase de predicci칩n es muy r치pida.\n","\n","* Debido a que solo se ven afectados por puntos cercanos al margen, funcionan bien con datos de alta dimensi칩n, incluso datos con m치s dimensiones que muestras.\n","\n","* Su integraci칩n con los m칠todos del kernel los hace muy vers치tiles, capaces de adaptarse a muchos tipos de datos.\n","\n","Sin embargo, las SVM tambi칠n tienen varias desventajas:\n","\n","* La escala con el n칰mero de muestras 洧녜 es $O[洧녜^{3}]$ en el peor de los casos, o $O[洧녜^{2}]$ para implementaciones eficientes. Para un gran n칰mero de muestras, el costo computacional puede resultar prohibitivo.\n","\n","* Los resultados dependen en gran medida de una elecci칩n adecuada para el par치metro de regularizaci칩n 洧냤. Este debe elegirse cuidadosamente a trav칠s de la validaci칩n cruzada, que puede ser costosa a medida que los conjuntos de datos aumentan de tama침o.\n","\n","* Los resultados no tienen una interpretaci칩n probabil칤stica directa. Esto se puede estimar mediante una validaci칩n cruzada interna, pero esta estimaci칩n adicional es costosa."]}]}