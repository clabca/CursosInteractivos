{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Wykx-ARBFoz0"},"source":["# **Aprendizaje supervisado**\n","# SL03. Regresión Lineal"]},{"cell_type":"code","metadata":{"id":"XL-XgV7PFoz3"},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import seaborn as sns; sns.set()\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-6TcygL3Fo0A"},"source":["## <font color='blue'>**Regresion lineal simple**</font>\n","\n","Comenzaremos con la regresión lineal más familiar, un ajuste de línea recta a los datos.\n","Un ajuste en línea recta es un modelo de la forma\n","$$\n","y = ax + b\n","$$\n","donde $a$ se conoce comúnmente como *pendiente*, y $b$ se conoce comúnmente como *intersección*.\n","\n","Considere los siguientes datos, que se encuentran dispersos sobre una línea con una pendiente de 2 y una intersección de -5:"]},{"cell_type":"code","metadata":{"id":"cZrvZpZMFo0C"},"source":["rng = np.random.RandomState(1)\n","x = 10 * rng.rand(500)\n","y = 2 * x - 5 + rng.randn(500)\n","plt.scatter(x, y, alpha=0.5)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_WjA97Q-Fo0I"},"source":["Usaremos el estimador de Scikit-Learn ``LinearRegression`` para ajustar la data y construir el modelo:"]},{"cell_type":"code","source":["x.shape"],"metadata":{"id":"07FOkrD_nXZU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x[:, np.newaxis].shape"],"metadata":{"id":"kPtxoiL0njkL"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AiEE7jvUFo0K"},"source":["from sklearn.linear_model import LinearRegression\n","model = LinearRegression(fit_intercept=True)\n","\n","model.fit(x[:, np.newaxis], y)\n","\n","xfit = np.linspace(0, 10, 1000)\n","yfit = model.predict(xfit[:, np.newaxis])\n","\n","plt.scatter(x, y, alpha=0.5)\n","plt.plot(xfit, yfit, c='r')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iZvOlKg_Fo0V"},"source":["La pendiente y la intersección de los datos están contenidos en los parámetros de ajuste del modelo, que en Scikit-Learn siempre están marcados con un guión bajo al final.\n","Aquí los parámetros relevantes son `` coef_`` e `` intercept_``:"]},{"cell_type":"code","metadata":{"id":"TSAoSyAgFo0V"},"source":["print(\"Pendiente:    \", model.coef_[0])\n","print(\"Intercepto:\", model.intercept_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ca2jDSGyFo0b"},"source":["Vemos que los resultados están muy cerca de las entradas, como podríamos esperar."]},{"cell_type":"markdown","metadata":{"id":"1fR_T54yFo0c"},"source":["Sin embargo, el estimador de ``LinearRegression`` es mucho más capaz que esto; además de ajustes simples en línea recta, también puede manejar modelos lineales multidimensionales de la forma\n","$$\n","y = a_0 + a_1 x_1 + a_2 x_2 + \\cdots\n","$$\n","donde hay multiples valores $x$.\n","Geométricamente, esto es similar a ajustar un plano a puntos en tres dimensiones, o ajustar un hiperplano a puntos en dimensiones más altas.\n","\n","La naturaleza multidimensional de tales regresiones las hace más difíciles de visualizar, pero podemos ver uno de estos ajustes en acción construyendo algunos datos de ejemplo, usando el operador de multiplicación de matrices de NumPy:"]},{"cell_type":"code","metadata":{"id":"7wzl118gFo0c"},"source":["rng = np.random.RandomState(1)\n","X = 10 * rng.rand(100, 3)\n","y = 0.5 + np.dot(X, [1.5, -2., 1.])\n","\n","model.fit(X, y)\n","print(model.intercept_)\n","print(model.coef_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QijrHke6Fo0i"},"source":["Aquí, los datos $ y $ se construyen a partir de tres valores $ x $ aleatorios, y la regresión lineal recupera los coeficientes utilizados para construir los datos.\n","\n","De esta manera, podemos usar el estimador único de ``LinearRegression``  para ajustar líneas, planos o hiperplanos a nuestros datos.\n","Todavía parece que este enfoque se limitaría a relaciones estrictamente lineales entre variables, pero resulta que también podemos relajar esto."]},{"cell_type":"markdown","metadata":{"id":"HRjAEjSZB_Tu"},"source":["## <font color='green'>Actividad 1</font>\n","\n","1. Realice un 5-fold cross validation con el modelo de regresión lineal con los datos anteriores (*X* e *y*).\n","2. Evalúe el error para los distintos conjuntos de test.\n","\n","¿Qué tan estable es el modelo?, ¿Cuán bueno es el error?\n"]},{"cell_type":"code","metadata":{"id":"RkaWNKnb6r2S"},"source":["# Tu código aquí\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j_iXVTxIWywU"},"source":["<font color='green'>Fin Actividad 1</font>"]},{"cell_type":"markdown","metadata":{"id":"NKhJgi28H1Uh"},"source":["## <font color='blue'>**Regresión de funciones base**</font>\n","\n","Sea $H$ una familia de funciones $X \\rightarrow Y$ y $T = \\{(x_1,y_1),.....(x_n,y_n)\\}$  un training set. Donde $x_i \\in \\mathbb{R}^D$ y $y_i \\in \\mathbb{R}$. Entonces se quiere seleccionar la función $g \\in H$ que minimiza el error dado por:\n","\n","$$ E(g) = \\frac{1}{2} \\sum_{i=1}^n (g(x_i) - y_i)^2$$\n","\n","En el caso de las funciones lineales nuestro H corresponde a:\n","\n","$$ \\{h(x): h(x) = w_o + \\sum_{i=1}^D w_ix_i \\}$$\n","\n","¿Cómo podemos expandir nuestro H pero que el mecanismo lineal siga\n","funcionando?\n","\n","Sea:\n"," $$\\phi_1,....,\\phi_M:\\mathbb{R}^D \\rightarrow \\mathbb{R}$$\n","\n","Entonces un elemento $h \\in H$ tiene la forma de:\n","$$ h(x) = w_o + \\sum_{i=1}^M w_i\\phi_i(x)$$\n","\n","Al conjunto de funciones $\\phi_i$, lo llamaremos funciones base.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Vy4apwL4Fo0i"},"source":["Un truco que puede utilizar para adaptar la regresión lineal a relaciones no lineales entre variables es transformar los datos de acuerdo con *funciones base*.\n","\n","La idea es tomar nuestro modelo lineal multidimensional:\n","$$\n","y = a_0 + a_1 x_1 + a_2 x_2 + a_3 x_3 + \\cdots\n","$$\n","y construir $x_1, x_2, x_3,$, de nuestra entrada unidimiensional $x$.\n","Esto es, $x_n = f_n(x)$, donde $f_n()$ es una funcion que transforma la data.\n","\n","Por ejemplo, si $f_n(x) = x^n$, nuestro modelo se transforma en una regresion polinomial:\n","$$\n","y = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + \\cdots\n","$$\n","Observe que este es *todavía un modelo lineal*: la linealidad se refiere al hecho de que los coeficientes $a_n$ nunca se multiplican ni se dividen entre sí.\n","Lo que hemos hecho efectivamente es tomar nuestros valores unidimensionales $x$ y proyectarlos en una dimensión superior, de modo que un ajuste lineal pueda encajar relaciones más complicadas entre $x$ y $y$."]},{"cell_type":"markdown","metadata":{"id":"Ke35C52jFo0j"},"source":["### Funciones de base polinomial\n","\n","Esta proyección polinomial es lo suficientemente útil como para estar integrada en Scikit-Learn, utilizando el transformador ``PolynomialFeatures``:"]},{"cell_type":"code","metadata":{"id":"de3bm9ADFo0k"},"source":["from sklearn.preprocessing import PolynomialFeatures\n","x = np.array([2, 3, 4])\n","poly = PolynomialFeatures(3, include_bias=False)\n","poly.fit_transform(x[:, None])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Snn3ghbdFo0o"},"source":["Vemos aquí que el transformador ha convertido nuestra matriz unidimensional en una matriz tridimensional tomando el exponente de cada valor.\n","Esta nueva representación de datos de mayor dimensión se puede conectar a una regresión lineal.\n"]},{"cell_type":"code","metadata":{"id":"2ZOwS-NEFo0p"},"source":["from sklearn.pipeline import make_pipeline\n","poly_model = make_pipeline(PolynomialFeatures(7),\n","                           LinearRegression())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wF-zQ2p7Fo0t"},"source":["Con esta transformación en su lugar, podemos usar el modelo lineal para ajustar relaciones mucho más complicadas entre $x$ y $y$.\n","Por ejemplo, aquí hay una onda sinusoidal con ruido:"]},{"cell_type":"code","metadata":{"id":"__pkfp5xFo0v"},"source":["rng = np.random.RandomState(1)\n","x = 10 * rng.rand(50)\n","y = np.sin(x) + 0.1 * rng.randn(50)\n","\n","poly_model.fit(x[:, np.newaxis], y) # Un espacio de 7 dimensiones mas un bias.\n","yfit = poly_model.predict(xfit[:, np.newaxis])\n","\n","plt.scatter(x, y)\n","plt.plot(xfit, yfit);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XNig9Ggrv6eE"},"source":["print(poly_model.steps[1][1].coef_) # 8 Coeficientes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gibmXmm1Fo0y"},"source":["Nuestro modelo lineal, mediante el uso de funciones de base polinomial de séptimo orden, puede proporcionar un ajuste excelente a estos datos no lineales."]},{"cell_type":"markdown","metadata":{"id":"x5ZjcDLiFo0z"},"source":["### Funciones base gaussianas\n","Por supuesto, son posibles otras funciones básicas.\n","Por ejemplo, un patrón útil es ajustar un modelo que no es una suma de bases polinomiales, sino una suma de bases gaussianas.\n","El resultado podría parecerse a la siguiente figura:"]},{"cell_type":"markdown","metadata":{"id":"bqet5JpUFo00"},"source":["![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.06-gaussian-basis.png?raw=1)\n","[figure source in Appendix](#Gaussian-Basis)"]},{"cell_type":"markdown","metadata":{"id":"zqy0Oq-qFo00"},"source":["Las regiones sombreadas en el gráfico son las funciones base escaladas y, cuando se suman, reproducen la curva suave a través de los datos.\n","Estas funciones de base gaussiana no están integradas en Scikit-Learn, pero podemos escribir un transformador personalizado que las creará, como se muestra aquí y se ilustra en la siguiente figura (los transformadores de Scikit-Learn se implementan como clases de Python; leer la fuente de Scikit-Learn es una buena forma de ver cómo se pueden crear):"]},{"cell_type":"markdown","metadata":{"id":"JaRECD1GNEV2"},"source":["## <font color='green'>Actividad 2</font>\n","\n","Entienda el código y describa la utilidad de cada uno de los metodos definidos.\n","\n","Ayuda:\n","1. Scikit-Learn nos proporciona dos excelentes clases base, TransformerMixin y BaseEstimator. Heredar de TransformerMixin asegura que todo lo que tenemos que hacer es escribir nuestros métodos de fit  y transform y obtenemos fit_transform de forma gratuita. La herencia de BaseEstimator garantiza que obtengamos get_params y set_params de forma gratuita. Dado que el método fit  no necesita hacer nada más que devolver el objeto en sí, todo lo que realmente necesitamos hacer después de heredar de estas clases es definir el método de transformación para nuestro transformador personalizado y obtenemos un transformador personalizado completamente funcional que puede ser sin problemas integrado con una canalización de scikit-learn! Fácil.\n","\n","2. Las bases gaussianas tienen la siguiente forma. $\\phi_j = wj*exp(-\\frac {(x - \\mu_j)^2}{2\\sigma^2})$\n","\n","3. Puede mirar el siguiente link. https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65\n"]},{"cell_type":"code","metadata":{"id":"tSLfNLtXFo01"},"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","\n","class GaussianFeatures(BaseEstimator, TransformerMixin):\n","    \"\"\"Uniformly spaced Gaussian features for one-dimensional input\"\"\"\n","\n","    def __init__(self, N, width_factor=2.0):\n","        self.N = N\n","        self.width_factor = width_factor\n","\n","    @staticmethod\n","    def _gauss_basis(x, y, width, axis=None):\n","        arg = (x - y) / width\n","        return np.exp(-0.5 * np.sum(arg ** 2, axis))\n","\n","    def fit(self, X, y=None):\n","        # create N centers spread along the data range\n","        self.centers_ = np.linspace(X.min(), X.max(), self.N)\n","        self.width_ = self.width_factor * (self.centers_[1] - self.centers_[0])\n","        return self\n","\n","    def transform(self, X):\n","        return self._gauss_basis(X[:, :, np.newaxis], self.centers_,\n","                                 self.width_, axis=1)\n","\n","gauss_model = make_pipeline(GaussianFeatures(20),\n","                            LinearRegression())\n","gauss_model.fit(x[:, np.newaxis], y)\n","yfit = gauss_model.predict(xfit[:, np.newaxis])\n","\n","plt.scatter(x, y)\n","plt.plot(xfit, yfit)\n","plt.xlim(0, 10);"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["help(GaussianFeatures)"],"metadata":{"id":"7G72985b8OSd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HWQcal1DW_aI"},"source":["<font color='green'>Fin Actividad 2</font>"]},{"cell_type":"code","metadata":{"id":"1q5zoHoGXjhP"},"source":[" #  Desarmando el cadigo de mas arriba.\n"," def _gauss_basis(x, y, width, axis=None):\n","        arg = (x - y) / width\n","        return np.exp(-0.5 * np.sum(arg ** 2, axis))\n","\n","def fite(X, N, width_factor=2.0):\n","        # create N centers spread along the data range\n","        centers_ = np.linspace(X.min(), X.max(), N)\n","        width_ = width_factor * (centers_[1] - centers_[0])\n","        return centers_, width_\n","\n","def transform(X, centers_, width_):\n","        return _gauss_basis(X[:, :, np.newaxis], centers_,\n","                                 width_, axis=1)\n","\n","centers_, width_ = fite(x[:, np.newaxis], 20)\n","results = transform(x[:, np.newaxis],centers_, width_ )\n","print(results)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"20AwZpVnFo06"},"source":["Ponemos este ejemplo aquí solo para aclarar que no hay nada mágico en las funciones de base polinómica: si tiene algún tipo de intuición en el proceso de generación de sus datos que le hace pensar que una base u otra podría ser apropiada, puede usarlas como bien."]},{"cell_type":"markdown","metadata":{"id":"lPDAg6daFo07"},"source":["## <font color='blue'>**Regularization**</font>\n","\n","La introducción de funciones base en nuestra regresión lineal hace que el modelo sea mucho más flexible, pero también puede conducir muy rápidamente a un ajuste excesivo.\n","Por ejemplo, si elegimos demasiadas funciones de base gaussiana, terminamos con resultados que no se ven tan bien:"]},{"cell_type":"code","metadata":{"id":"Ooj7EXLAFo08"},"source":["model = make_pipeline(GaussianFeatures(30),\n","                      LinearRegression())\n","model.fit(x[:, np.newaxis], y)\n","\n","plt.scatter(x, y)\n","plt.plot(xfit, model.predict(xfit[:, np.newaxis]))\n","\n","plt.xlim(0, 10)\n","plt.ylim(-1.5, 1.5);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oD24wzl1Fo0-"},"source":["Con los datos proyectados a la base de 30 dimensiones, el modelo tiene demasiada flexibilidad y llega a valores extremos entre ubicaciones donde está limitado por los datos.\n","Podemos ver la razón de esto si graficamos los coeficientes de las bases gaussianas con respecto a sus ubicaciones:"]},{"cell_type":"code","metadata":{"id":"q2nTwafcFo1B"},"source":["def basis_plot(model, title=None):\n","    fig, ax = plt.subplots(2, sharex=True)\n","    model.fit(x[:, np.newaxis], y)\n","    ax[0].scatter(x, y)\n","    ax[0].plot(xfit, model.predict(xfit[:, np.newaxis]))\n","    ax[0].set(xlabel='x', ylabel='y', ylim=(-1.5, 1.5))\n","\n","    if title:\n","        ax[0].set_title(title)\n","\n","    ax[1].plot(model.steps[0][1].centers_,\n","               model.steps[1][1].coef_)\n","    ax[1].set(xlabel='basis location',\n","              ylabel='coefficient',\n","              xlim=(0, 10))\n","\n","model = make_pipeline(GaussianFeatures(30), LinearRegression())\n","basis_plot(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iW8A_CpHwX8O"},"source":["print(model.steps[1][1].coef_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FwEFnsYOFo1F"},"source":["El panel inferior de esta figura muestra la amplitud de la función base en cada ubicación.\n","Este es un comportamiento de sobreajuste típico cuando las funciones base se superponen: los coeficientes de las funciones base adyacentes explotan y se cancelan entre sí.\n","Sabemos que tal comportamiento es problemático, y sería bueno si pudiéramos limitar explícitamente tales picos en el modelo penalizando los valores grandes de los parámetros del modelo.\n","Esta penalización se conoce como *regularización* y se presenta en varias formas."]},{"cell_type":"markdown","metadata":{"id":"C8-FaSptFo1G"},"source":["### Ridge regression ($L_2$ Regularization)\n","\n","Quizás la forma más común de regularización se conoce como *ridge regression* o $ L_2 $ *regularización*, a veces también llamada *regularización de Tikhonov*.\n","Esto procede penalizando la suma de cuadrados (norma L2) de los coeficientes del modelo; en este caso, la penalización en el ajuste del modelo sería\n","$$\n","P = \\alpha\\sum_{n=1}^N \\theta_n^2\n","$$\n","donde $\\alpha$ es un parametro libre que controla la fuerza de la penalidad.\n"]},{"cell_type":"code","metadata":{"id":"iOGfxW9aFo1H"},"source":["from sklearn.linear_model import Ridge\n","model = make_pipeline(GaussianFeatures(30), Ridge(alpha=0.1))\n","basis_plot(model, title='Ridge Regression')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mtRxIEHNFo1J"},"source":["El parámetro $\\alpha$ es esencialmente una perilla que controla la complejidad del modelo resultante.\n","En el límite $\\alpha\\ \\to 0$, recuperamos el resultado de regresión lineal estándar; en el límite $\\alpha \\to \\infty$, se suprimirán todas las respuestas del modelo.\n","Una ventaja de la regresión de crestas en particular es que se puede calcular de manera muy eficiente, a un costo computacional apenas mayor que el modelo de regresión lineal original."]},{"cell_type":"markdown","metadata":{"id":"dOtctO-6Fo1L"},"source":["### Lasso regression ($L_1$ regularization)\n","\n","Otro tipo de regularización muy común se conoce como lazo, e implica penalizar la suma de valores absolutos (1-norma) de los coeficientes de regresión:\n","$$\n","P = \\alpha\\sum_{n=1}^N |\\theta_n|\n","$$\n","Aunque esto es conceptualmente muy similar a la regresión de crestas, los resultados pueden diferir sorprendentemente: por ejemplo, debido a razones geométricas, la regresión de lazo tiende a favorecer *modelos dispersos* cuando es posible: es decir, preferentemente establece los coeficientes del modelo exactamente a cero.\n","\n","Podemos ver este comportamiento al duplicar la figura de regresión de la cresta, pero usando coeficientes normalizados L1:"]},{"cell_type":"code","metadata":{"id":"Z71tG1yKFo1L"},"source":["from sklearn.linear_model import Lasso\n","model = make_pipeline(GaussianFeatures(30), Lasso(alpha=0.001))\n","basis_plot(model, title='Lasso Regression')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3uryXxF0Fo1P"},"source":["Con la penalización por regresión de lazo, la mayoría de los coeficientes son exactamente cero, y el comportamiento funcional está modelado por un pequeño subconjunto de las funciones básicas disponibles.\n","Al igual que con la regularización de crestas, el parámetro $\\alpha$ ajusta la fuerza de la penalización y debe determinarse mediante, por ejemplo, validación cruzada."]},{"cell_type":"markdown","metadata":{"id":"VSoyoM-LBjOn"},"source":["## <font color='green'>Actividad 3</font>\n","\n","Usaremos el *Combined Cycle Power Plant Data Set*.\n","\n","1. Implementa distintos modelos de regresión para predecir el Energy Output (EP) de la planta y compare sus resultados. Los parametros entregados son los siguientes:\n","  * Ambient Temperature (AT)\n","  * Exhaust Vaucum (V)\n","  * Ambient Pressure (AP)\n","  * Relative Humidity (RH)\n","\n","2. Grafique sus resultados.\n","3. Calcular el coeficiente de correlación de Pearson"]},{"cell_type":"code","metadata":{"id":"u2VC5PQhl3FP"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_path = '/content/drive/MyDrive/Diplomado UDD-BCH 2023/Material_clases_CD_AD_2023/M06-Aprendizaje_supervisado/files/powerplant.csv'"],"metadata":{"id":"j5tPNe5VCs6m"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wPA6U71zBjOo"},"source":["# Tu código aquí"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5BLSbKmmXHZi"},"source":["<font color='green'>Fin Actividad 3</font>"]},{"cell_type":"markdown","metadata":{"id":"LhgxiqKR8xQU"},"source":["## <font color='green'>Actividad 4</font>\n","\n","La regresión lineal se estudió a través del método de los minimos cuadrados. En esta actividad le proponemos el desafío de implementar la regresión lineal a traves del algoritmo de gradiente descendente.\n","\n","Para esto uste debe aprender cómo funciona  el algoritmo de descenso de gradientes e implementarlo desde cero en Python y aplicarlo al problema de regresión lineal.\n","\n","Para el desarrollo de la tarea, sugerimos estudiar el siguiente tutorial:\n","\n","https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931"]},{"cell_type":"code","metadata":{"id":"LtVYEGA2seoj"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import r2_score\n","from numpy import c_\n","\n","%matplotlib inline\n","\n","data = pd.read_csv(\"/content/drive/MyDrive/Diplomado UDD-BCH 2023/Material_clases_CD_AD_2023/M06-Aprendizaje_supervisado/files/powerplant.csv\",header=0)\n","data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nVQrMPSwsv5M"},"source":["# Tu código aquí\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aYkuFw8Z95zD"},"source":["<font color='green'>Fin Actividad 4</font>"]}]}