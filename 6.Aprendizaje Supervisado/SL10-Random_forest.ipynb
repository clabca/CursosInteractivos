{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"markdown","metadata":{"id":"PIGL_DdNWqqj"},"source":["# **Aprendizaje supervisado**\n","# SL10. Random forest"]},{"cell_type":"markdown","metadata":{"id":"D4MguWrAWqqk"},"source":["Echaremos un vistazo a la motivación de un poderoso algoritmo: un algoritmo no paramétrico llamado **Random Forest**.\n","Random Forest es un ejemplo de un método de **ensamblado** , lo que significa que se basa en la agregación de los resultados de un conjunto de estimadores más simples.\n","El resultado algo sorprendente con tales métodos de conjunto es que la suma puede ser mayor que las partes: es decir, ¡un voto mayoritario entre varios estimadores puede terminar siendo mejor que cualquiera de los estimadores individuales que votan!\n","Veremos ejemplos de esto en las siguientes secciones.\n","Comenzamos con las importaciones estándar:"]},{"cell_type":"code","metadata":{"id":"lZ2tAUlnWqqm"},"source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns; sns.set()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"54LmoV3nWqqw"},"source":["## <font color='blue'>**Random Forest**</font>"]},{"cell_type":"markdown","metadata":{"id":"xQnBKqK7Wqqw"},"source":["Random forest es un ejemplo de un **ensamblado** construido sobre árboles de decisión.\n","Por esta razón, comenzaremos discutiendo los propios árboles de decisión.\n","\n","1. Los árboles de decisión a menudo imitan el pensamiento a nivel humano, por lo que es muy sencillo comprender los datos y hacer algunas buenas interpretaciones.\n","\n","2. Los árboles de decisión le hacen ver la lógica de los datos a interpretar (no como los algoritmos de caja negra como SVM, ANN, etc.).\n","\n","3. Un árbol de decisión es un árbol donde cada nodo representa una característica (atributo), cada vínculo (rama) representa una decisión (regla) y cada hoja representa un resultado (valor categórico o continuo).\n","\n","Los árboles de decisión son formas extremadamente intuitivas de clasificar o etiquetar objetos: simplemente haga una serie de preguntas diseñadas para concentrarse en la clasificación.\n","Por ejemplo, si quisiera construir un árbol de decisiones para clasificar un animal con el que se encuentra durante una caminata, puede construir el que se muestra aquí:"]},{"cell_type":"markdown","metadata":{"id":"kaZCZElBWqqx"},"source":["![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.08-decision-tree.png?raw=1)\n"]},{"cell_type":"markdown","metadata":{"id":"Ha76PkRhWqqy"},"source":["La división binaria hace que esto sea extremadamente eficiente: en un árbol bien construido, cada pregunta reducirá el número de opciones a aproximadamente la mitad, reduciendo muy rápidamente las opciones incluso entre un gran número de clases.\n","El truco, por supuesto, está en decidir qué preguntas hacer en cada paso.\n","En las implementaciones de aprendizaje automático de árboles de decisión, las preguntas generalmente toman la forma de divisiones alineadas con el eje en los datos: es decir, cada nodo del árbol divide los datos en dos grupos utilizando un valor de corte dentro de una de las características.\n","Veamos ahora un ejemplo de esto."]},{"cell_type":"markdown","metadata":{"id":"p_EjD8-JWqqz"},"source":["## Creando Árboles de Decisión\n","\n","Considere los siguientes datos bidimensionales, que tienen una de cuatro etiquetas de clase:"]},{"cell_type":"code","metadata":{"id":"4ptrWrByWqq1"},"source":["from sklearn.datasets import make_blobs\n","\n","X, y = make_blobs(n_samples=300, centers=4,\n","                  random_state=0, cluster_std=1.0)\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ayVSYG16Wqq7"},"source":["Un árbol de decisión simple construido sobre estos datos dividirá iterativamente los datos a lo largo de uno u otro eje de acuerdo con algún criterio cuantitativo, y en cada nivel asignará la etiqueta de la nueva región de acuerdo con un voto mayoritario de puntos dentro de ella.\n","Esta figura presenta una visualización de los primeros cuatro niveles de un clasificador de árbol de decisión para estos datos:"]},{"cell_type":"markdown","metadata":{"id":"NJrF7cPQWqq8"},"source":["![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.08-decision-tree-levels.png?raw=1)\n"]},{"cell_type":"markdown","metadata":{"id":"vPAjc84iWqq9"},"source":["Observe que después de la primera división, todos los puntos de la rama superior permanecen sin cambios, por lo que no es necesario subdividir más esta rama.\n","A excepción de los nodos que contienen todos de un color, en cada nivel * cada * región se divide nuevamente a lo largo de una de las dos características."]},{"cell_type":"markdown","metadata":{"id":"4rGrC56yWqq-"},"source":["Este proceso de ajustar un árbol de decisiones a nuestros datos se puede realizar en Scikit-Learn con el estimador `` DecisionTreeClassifier '':"]},{"cell_type":"code","metadata":{"id":"NO84XKWgWqq_"},"source":["from sklearn.tree import DecisionTreeClassifier\n","tree = DecisionTreeClassifier().fit(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rZX0VMy5WqrE"},"source":["Escribamos una función de utilidad rápida para ayudarnos a visualizar la salida del clasificador:"]},{"cell_type":"code","source":["a = np.array([[1, 2, 3]])\n","b = np.array([[10, 20, 30, 40]])\n","c = np.array([[1, 2, 3],\n","             [4, 5, 6]\n","             ])"],"metadata":{"id":"NJ_3f5J81mbd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a"],"metadata":{"id":"A9D1jZKk11co"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a.shape"],"metadata":{"id":"CEHbUFUU124P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x1, y1 = np.meshgrid(a, b)\n","print(x1)\n","print(y1)"],"metadata":{"id":"qdPnOI7b2I0N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["c.ravel()"],"metadata":{"id":"tnMKEDeu14gU"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RT6Bhr2uWqrG"},"source":["def visualize_classifier(model, X, y, ax=None, cmap='rainbow'):\n","    ax = ax or plt.gca()\n","\n","    # Plot the training points\n","    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,\n","               clim=(y.min(), y.max()), zorder=3)\n","    ax.axis('tight')\n","    ax.axis('off')\n","    xlim = ax.get_xlim()\n","    ylim = ax.get_ylim()\n","\n","    # fit the estimator\n","    model.fit(X, y)\n","    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n","                         np.linspace(*ylim, num=200))\n","    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n","\n","    # Create a color plot with the results\n","    n_classes = len(np.unique(y))\n","    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n","                           levels=np.arange(n_classes + 1) - 0.5,\n","                           cmap=cmap, clim=(y.min(), y.max()),\n","                           zorder=1)\n","\n","    ax.set(xlim=xlim, ylim=ylim)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TeEyJD-UWqrK"},"source":["Ahora podemos examinar cómo se ve la clasificación del árbol de decisión:"]},{"cell_type":"code","metadata":{"id":"AdRl6ZJSWqrM"},"source":["visualize_classifier(DecisionTreeClassifier(), X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wQDkvbbiWqrW"},"source":["Observe que a medida que aumenta la profundidad, tendemos a obtener regiones de clasificación de formas muy extrañas; por ejemplo, a una profundidad de cinco, hay una región púrpura alta y delgada entre las regiones amarilla y azul.\n","Está claro que esto es menos un resultado de la verdadera distribución intrínseca de los datos y más un resultado de las propiedades particulares de muestreo o ruido de los datos.\n","Es decir, este árbol de decisiones, incluso a solo cinco niveles de profundidad, claramente sobrepasa nuestros datos."]},{"cell_type":"markdown","metadata":{"id":"KKHNzqWEWqrX"},"source":["## Árboles de Decisión y Overfitting\n","\n","Tal sobreajuste resulta ser una propiedad general de los árboles de decisión: es muy fácil profundizar demasiado en el árbol y, por lo tanto, ajustar los detalles de los datos particulares en lugar de las propiedades generales de las distribuciones de las que se extraen.\n","Otra forma de ver este ajuste excesivo es observar modelos entrenados en diferentes subconjuntos de datos; por ejemplo, en esta figura entrenamos dos árboles diferentes, cada uno en la mitad de los datos originales:"]},{"cell_type":"markdown","metadata":{"id":"cjTNIsPVWqrY"},"source":["![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.08-decision-tree-overfitting.png?raw=1)"]},{"cell_type":"markdown","metadata":{"id":"lap-vRjqWqrZ"},"source":["Está claro que en algunos lugares, los dos árboles producen resultados consistentes (por ejemplo, en las cuatro esquinas), mientras que en otros lugares, los dos árboles dan clasificaciones muy diferentes (por ejemplo, en las regiones entre dos grupos).\n","La observación clave es que las inconsistencias tienden a ocurrir donde la clasificación es menos segura y, por lo tanto, al usar información de * ambos * de estos árboles, ¡podríamos obtener un mejor resultado!"]},{"cell_type":"markdown","metadata":{"id":"QspYkG_7Wqra"},"source":["Si está ejecutando este cuaderno en vivo, la siguiente función le permitirá mostrar interactivamente los ajustes de árboles entrenados en un subconjunto aleatorio de los datos:"]},{"cell_type":"markdown","metadata":{"id":"HQKwvH4nWqrg"},"source":["Así como el uso de información de dos árboles mejora nuestros resultados, podríamos esperar que el uso de información de muchos árboles mejore nuestros resultados aún más."]},{"cell_type":"markdown","metadata":{"id":"7dxSmCOotgJ9"},"source":["## Algoritmos\n","\n","1. CART (árboles de clasificación y regresión) → utiliza el índice de Gini (clasificación) como métrica.\n","\n","2. ID3 (dicotomizador iterativo 3) → utiliza la función de entropía y la ganancia de información como métricas.\n","\n","**Entropía:** La entropía de un conjunto de datos, es una medida de la impureza o desorden, del conjunto de datos La entropía también puede pensarse, como una medida de incertidumbre. Deberíamos intentar minimizar la entropía. El objetivo de los modelos de aprendizaje automático es reducir la incertidumbre o la entropía, en la medida de lo posible.\n","\n","$$ H(S) = \\sum_{s\\in S}-p(s)log_2 p(s)$$\n","Donde s son las clases de S.\n","\n","**Ganancia de información:** La ganancia de información es una medida de cuánta información nos brinda una característica sobre las clases. El algoritmo de árboles de decisión siempre intentará maximizar la ganancia de información. Característica, que divide perfectamente los datos, debe dar la máxima información. Una función, con la mayor ganancia de información, se utilizará para dividir primero.\n","\n","$$ IG(S,a) = H(S) - H(S|a)$$\n","Como calculamos  $IG(S,a)$. Supongamos un conjunto donde 13 valores son 0 y 7 valores son 1. Luego, tenemos una variable $a$ que tiene dos valores valor0 y valor 1. Posteriormente separamos por esta variable obteniendo los siguientes conjuntos. 8(7|1) y 12(6|6). ¿Cuanto vale IG(S,a)?\n","\n","$$ IG(S,a) = H(\\frac{13}{20}, \\frac{7}{20}) - (H(\\frac{7}{8}, \\frac{1}{8}) + H(\\frac{6}{12}, \\frac{6}{12}))$$\n","\n","**Indice de Gini:** El índice de Gini, también conocido como impureza de Gini, es una medida de entropía y calcula  de probabilidad de un elemento que pertenece a una categoria. se clasifica incorrectamente cuando se selecciona al azar. Si el elemento pertenece a la categoria $i$ la probabilidad de ser mal clasificado esta dado por $ \\sum_{i \\neq k} p_k = 1 - pi$\n","\n"," Gini Index (GI) = $$ \\sum_{i \\in S} p_i \\sum_{j \\neq i} p_j = \\sum_{i \\in S} p_i(1-p_i) = \\sum_{i \\in S} p_i - \\sum_{i \\in S} p_i^2 =  1 - \\sum_{i \\in S} (P_i)^2$$, por ejemplo supongamos que tenemos 4 bolas rojas y 0 azul. EL indice de Gini es:\n","\n","$$ I_G = 1 - (1^2 + 0^2)=1$$\n","Caso 2 y 2 tenemos:\n","$$ I_G = 1 - ((\\frac{1}{2})^2 + (\\frac{1}{2})^2)=0.5$$\n","\n","Caso 3 rojas una azul\n","$$ I_G = 1 - ((\\frac{3}{4})^2 + (\\frac{1}{4})^2)=0.375$$\n","\n","Al igual que con la entropía para estimar la mejor separación, se utiliza la ganancia del coeficiente de Gini."]},{"cell_type":"markdown","metadata":{"id":"uuKtQyrxtl3l"},"source":["## <font color='green'>Actividad 1</font>\n","\n","Construya un arbol de decisión con los datos proporcionados e interprete este. Apóyese en los códigos siguientes.\n","\n","\n","\n","```\n","from sklearn.tree import DecisionTreeClassifier\n","```\n","\n","Considere utilizar variables dummy para poder realizar la separación.\n","\n","\n","\n","```\n","one_hot_data = pd.get_dummies\n","```\n","\n","Y utilice el siguiente código para visualizar el resultado del arbol.\n","\n","\n","\n","```\n","# Export/Print un arbol de decisión en formato .dot.\n","print(tree.export_graphviz(clf_train, None))\n","\n","# Crea la data.\n","dot_data = tree.export_graphviz(clf_train, out_file=None, feature_names=list(one_hot_data.columns.values),\n","                                class_names=['Not_Play', 'Play'], rounded=True, filled=True) #Gini o Entropia.\n","#Creaamos el grafico.\n","graph = pydotplus.graph_from_dot_data(dot_data)\n","\n","# Mostramos el gráfico.\n","Image(graph.create_png())\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"w7rMmPOQtgbT"},"source":["## importemos dependencias\n","from sklearn import tree\n","import pandas as pd\n","import pydotplus\n","from IPython.display import Image\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h2lk-EsEtgg6"},"source":["# Creemos el dataset\n","\n","golf_df = pd.DataFrame()\n","\n","#agreguemos outlook\n","golf_df['Outlook'] = ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy',\n","                     'overcast', 'sunny', 'sunny', 'rainy', 'sunny', 'overcast',\n","                     'overcast', 'rainy']\n","\n","#agreguemos temperature\n","golf_df['Temperature'] = ['hot', 'hot', 'hot', 'mild', 'cool', 'cool', 'cool',\n","                         'mild', 'cool', 'mild', 'mild', 'mild', 'hot', 'mild']\n","\n","#agreguemos humidity\n","golf_df['Humidity'] = ['high', 'high', 'high', 'high', 'normal', 'normal', 'normal',\n","                      'high', 'normal', 'normal', 'normal', 'high', 'normal', 'high']\n","\n","#agreguemos windy\n","golf_df['Windy'] = ['false', 'true', 'false', 'false', 'false', 'true', 'true',\n","                   'false', 'false', 'false', 'true', 'true', 'false', 'true']\n","\n","#Se juega o no play?\n","golf_df['Play'] = ['no', 'no', 'yes', 'yes', 'yes', 'no', 'yes', 'no', 'yes', 'yes', 'yes',\n","                  'yes', 'yes', 'no']\n","\n","\n","#Print/show the new data\n","print(golf_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L1kaAq44t23P"},"source":["# Tú codigo aquí"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A5MIRcHdt_iV"},"source":["<font color='green'>Fin Actividad 1</font>"]},{"cell_type":"markdown","metadata":{"id":"dKOUTjdwWqrh"},"source":["## <font color='blue'>**Ensamblado de Estimadores: Random Forest**</font>\n","\n","Ensemble es un concepto de Machine Learning en el que la idea es entrenar múltiples modelos utilizando el mismo algoritmo de aprendizaje. Los conjuntos participan en un grupo más grande de métodos, llamados multiclasificadores, donde un conjunto de cientos o miles de modelos con un objetivo común se fusionan para resolver el problema.\n","\n","Las principales causas de error en el aprendizaje se deben al ruido, el Bias y el variance. EL Ensemble ayuda a minimizar estos factores. Estos métodos están diseñados para mejorar la estabilidad y precisión de los algoritmos de aprendizaje automático. Las combinaciones de múltiples clasificadores disminuyen la varianza, especialmente en el caso de clasificadores inestables, y pueden producir una clasificación más confiable que un solo clasificador.\n","\n","1. En un Bagging y Boosting, tenemos N weak learners. Bagging and Boosting consigue N learners al generar datos adicionales en la etapa de entrenamiento. Se producen N nuevos conjuntos de datos de entrenamiento mediante un muestreo aleatorio con reemplazo del conjunto original. Al muestrear con reemplazo, algunas observaciones pueden repetirse en cada nuevo conjunto de datos de entrenamiento.\n","\n","![BaggingBoosting_1](https://drive.google.com/uc?export=view&id=1kfwiVz97sbjxwbEQEjpt_IEIpDE98G1b)\n","\n","\n","2. En el caso del bagging, cualquier elemento tiene la misma probabilidad de aparecer en un nuevo conjunto de datos. Sin embargo, para Boosting las observaciones están ponderadas y, por lo tanto, algunas de ellas participarán en los nuevos conjuntos con más frecuencia:\n","\n","![BaggingBoosting_2](https://drive.google.com/uc?export=view&id=1WJDzIDX-Nc-5t_qWDdxIQS7UH2SWGkkj)\n","\n","3. En este punto, comenzamos a abordar la principal diferencia entre los dos métodos. Si bien la etapa de entrenamiento es paralela para Bagging (es decir, cada modelo se construye de forma independiente), Boosting construye al nuevo alumno de forma secuencial:\n","\n","![BaggingBoosting_3](https://drive.google.com/uc?export=view&id=1sLd8mNEryln1om7SmKwbQULXv0EAyekT)\n","\n","En los algoritmos boosting, cada clasificador se entrena con datos, teniendo en cuenta el resultado de los clasificadores anteriores. Después de cada paso de entrenamiento, los pesos se redistribuyen. Los datos mal clasificados aumentan su ponderación para enfatizar los casos más difíciles. De esta forma, los learners posteriores se centrarán en ellos durante su entrenamiento.\n","\n","4. Para predecir la clase de nuevos datos, solo necesitamos aplicar los N learners a las nuevas observaciones. En Bagging, el resultado se obtiene promediando las respuestas de los N learners (o el voto de la mayoría). Sin embargo, Boosting asigna un segundo conjunto de ponderaciones, esta vez para los clasificadores N, con el fin de obtener un promedio ponderado de sus estimaciones. En la etapa de entrenamiento Boosting, el algoritmo asigna pesos a cada modelo resultante. A un learners con un buen resultado de clasificación en los datos de entrenamiento se le asignará un peso más alto que a uno deficiente. Por lo tanto, al evaluar a un nuevo learners, Boosting también debe realizar un seguimiento de los errores de los learners.\n","\n","![BaggingBoosting_4](https://drive.google.com/uc?export=view&id=1eYDalb6ZBzvKhFWM89Ku-3BwrUzduUYq)\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Vdqm_1XvWqri"},"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import BaggingClassifier # Usa muestreo boostrap\n","\n","tree = DecisionTreeClassifier() # Learner\n","bag = BaggingClassifier(tree, n_estimators=100, max_samples=0.8, # Le pasamos los learners\n","                        random_state=1)\n","\n","bag.fit(X, y)\n","visualize_classifier(bag, X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CBrg78j7Wqrn"},"source":["En este ejemplo, hemos aleatorizado los datos ajustando cada estimador con un subconjunto aleatorio del 80% de los puntos de entrenamiento.\n","En la práctica, los árboles de decisión se aleatorizan de manera más efectiva inyectando algo de estocasticidad en la forma en que se eligen las divisiones: de esta manera, todos los datos contribuyen al ajuste cada vez, pero los resultados del ajuste aún tienen la aleatoriedad deseada.\n","Por ejemplo, al determinar qué característica dividir, el árbol aleatorio puede seleccionar entre las principales características.\n","Puede leer más detalles técnicos sobre estas estrategias de aleatorización en la [documentación de Scikit-Learn] (http://scikit-learn.org/stable/modules/ensemble.html#forest) y en las referencias.\n","\n","En Scikit-Learn, este conjunto optimizado de árboles de decisión aleatorios se implementa en el estimador `` RandomForestClassifier '', que se encarga de toda la aleatorización automáticamente.\n","Todo lo que necesita hacer es seleccionar una serie de estimadores, y muy rápidamente (en paralelo, si lo desea) se ajustará al conjunto de árboles:"]},{"cell_type":"code","metadata":{"id":"YDY5qpydWqrp"},"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","model = RandomForestClassifier(n_estimators=100, random_state=0)\n","visualize_classifier(model, X, y);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fJLrfElQWqrs"},"source":["Vemos que al promediar más de 100 modelos perturbados aleatoriamente, terminamos con un modelo general que está mucho más cerca de nuestra intuición sobre cómo se debe dividir el espacio de parámetros."]},{"cell_type":"markdown","metadata":{"id":"TUEJgOwvWqrt"},"source":["## Random Forest para Regressión\n","\n","En la sección anterior consideramos los random forest dentro del contexto de la clasificación.\n","Random forest también pueden funcionar en el caso de regresión (es decir, variables continuas en lugar de categóricas). El estimador a utilizar para esto es el `` RandomForestRegressor '', y la sintaxis es muy similar a la que vimos anteriormente.\n","\n","Considere los siguientes datos, extraídos de la combinación de una oscilación rápida y lenta:"]},{"cell_type":"code","metadata":{"id":"mDx1ETIbWqru"},"source":["rng = np.random.RandomState(42)\n","x = 10 * rng.rand(200)\n","\n","def model(x, sigma=0.3):\n","    fast_oscillation = np.sin(5 * x)\n","    slow_oscillation = np.sin(0.5 * x)\n","    noise = sigma * rng.randn(len(x))\n","\n","    return slow_oscillation + fast_oscillation + noise\n","\n","y = model(x)\n","plt.errorbar(x, y, 0.3, fmt='o');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7ZUd9_WwWqrx"},"source":["Usando el regresor de random forest, podemos encontrar la curva de mejor ajuste de la siguiente manera:"]},{"cell_type":"code","metadata":{"id":"XWePTsDzWqrz"},"source":["from sklearn.ensemble import RandomForestRegressor\n","forest = RandomForestRegressor(200)\n","forest.fit(x[:, None], y)\n","\n","xfit = np.linspace(0, 10, 1000)\n","yfit = forest.predict(xfit[:, None])\n","ytrue = model(xfit, sigma=0)\n","\n","plt.errorbar(x, y, 0.3, fmt='o', alpha=0.5)\n","plt.plot(xfit, yfit, '-r'); # Este es el fit\n","plt.plot(xfit, ytrue, '-k', alpha=0.5);  # y este es el original"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WoBfKOZzWqr3"},"source":["Aquí, el modelo real se muestra en la curva gris suave, mientras que el modelo de random forest se muestra en la curva roja irregular.\n","Como puede ver, el modelo de random forest no paramétrico es lo suficientemente flexible como para ajustarse a los datos de períodos múltiples, ¡sin que tengamos que especificar un modelo de períodos múltiples!"]},{"cell_type":"markdown","metadata":{"id":"ZKBveiduWqr3"},"source":["## <font color='green'>Actividad 2</font>\n","\n","Clasificación de digitos. Esta actividad tiene por objetivo implementar un clasificador de digitos escritos a mano utilizando random forest.\n","\n","1. Explore el data se y visualice datos. Puede utilizar el siguiente código para ver las imagenes. ¿Cuales son las dimensiones de la imágen?, ¿Entre que valores se mueve?\n","\n","```\n","ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n","```\n","2. Entrene un clasificador utilizando **RandomForestClassifier**. Explore algunos parámetros como por ejemplo.\n","\n","* __max_depth__: La profundidad máxima del árbol. Si es None, los nodos se expanden hasta que todas las hojas sean puras o hasta que todas las hojas contengan menos de min_samples_split muestras.\n","\n","* __n_estimators__: La cantidad de árboles a considerar.\n","\n","* __max_features__: La cantidad de características que se deben considerar al buscar la mejor división:\n","<br>\n","\n","3. Evalue los resultados a través de una matriz de confusión y ponga el resultado de una mariz en un heatmap para una mejor visualización.\n"]},{"cell_type":"code","metadata":{"id":"XflhisjKWqr4"},"source":["# Cargando y visualizando el dataset.\n","from sklearn.datasets import load_digits\n","digits = load_digits()\n","digits.keys()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ivgaJpioWqr8"},"source":["Para recordarnos lo que estamos viendo, visualizaremos los primeros puntos de datos:"]},{"cell_type":"code","metadata":{"id":"Kx8Zt5TUWqr9"},"source":["# Tú codigo aquí"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nRUA6NhLWqsB"},"source":["Podemos clasificar rápidamente los dígitos usando un random forest de la siguiente manera:"]},{"cell_type":"markdown","metadata":{"id":"5VaiGpg0WqsN"},"source":["<font color='green'>Fin Actividad 2</font>"]},{"cell_type":"markdown","metadata":{"id":"rX5w5Q1FWqsO"},"source":["## Resumen del Método Random Forest:\n","\n","Esta sección contiene una breve introducción al concepto de *ensamblados*, y en particular random forest, un conjunto de árboles de decisión aleatorios.\n","Random Forest es un método poderoso con varias ventajas:\n","\n","- Tanto el entrenamiento como la predicción son muy rápidos, debido a la simplicidad de los árboles de decisión subyacentes. Además, ambas tareas se pueden paralelizar directamente, porque los árboles individuales son entidades completamente independientes.\n","- Los árboles múltiples permiten una clasificación probabilística: un voto mayoritario entre los estimadores da una estimación de la probabilidad (se accede en Scikit-Learn con el método ``predict_proba ()``).\n","- El modelo no paramétrico es extremadamente flexible y, por lo tanto, puede funcionar bien en tareas que no están ajustadas por otros estimadores.\n","\n","Una desventaja principal de Random Forest es que los resultados no se pueden interpretar fácilmente: es decir, si desea sacar conclusiones sobre el *significado* del modelo de clasificación,  Random Forest pueden no ser la mejor opción."]}]}